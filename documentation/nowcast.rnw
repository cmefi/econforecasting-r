\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{float}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{appendix}
\graphicspath{ {.} }
\newcommand{\vv}[1]{\textcolor{black}{\mathbf{#1}}}
\definecolor{econgreen}{RGB}{55, 91, 1}

\geometry{left=2.6cm, right = 2.6cm, top = 3cm, bottom = 3cm}

\fancypagestyle{plain}{
	\let\oldheadrule\headrule% Copy \headrule into \oldheadrule
	\renewcommand{\headrule}{\color{econgreen}\oldheadrule}
	\lhead{\small{\textcolor{black}{\leftmark}}}
	%\chead{}
	\rhead{\small{\textcolor{black}{\thepage}}}
	\lfoot{}
	\cfoot{}
	\rfoot{}
	\renewcommand{\headrulewidth}{0.5pt}
	%\renewcommand{\footrulewidth}{0.5pt}
}\pagestyle{plain}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\thispagestyle{empty}
\newgeometry{left=5cm, top=5cm} %defines the geometry for the titlepage
\pagecolor{econgreen}
\noindent
\includegraphics[width=2cm]{cmefi_short.png} \\[-1em]
\color{white}
\makebox[0pt][l]{\rule{1.3\textwidth}{1pt}}
\par
\noindent
%\textbf{\textsf{A Macroeconomic Nowcasting Model}} 
%\vskip5cm
{\Huge \textsf{A Nowcasting Model for Time Series with Ragged-Edge Data}}
\vskip\baselineskip
\noindent
\textsf{Model Run Date: \Sexpr{format(Sys.Date(), '%B %d, %Y')}}\\
\textsf{Charles Y. | charles@cmefi.com}
\restoregeometry % restores the geometry
\nopagecolor% Use this to restore the color pages to white
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


<<echo=F,results='hide',message=F, warning=F>>=
    library(tidyverse)
    library(xtable)

    # Settings for PDF compilation
    knitr::opts_chunk$set(
      echo = F, results = 'asis',
      fig.width = 10, fig.height = 4, out.width = '5in', out.height = '2in',
      fig.pos = 'H', fig.align = 'center', warnings = F, message = F
      )
    options(xtable.include.rownames = F)
    options(xtable.size = 'footnotesize')
    options(xtable.table.placement = 'H')
    
    rds = readRDS('D:/Onedrive/__Projects/econforecasting/nowcast.rds')

@

\tableofcontents
\newpage
\listoftables
\listoffigures
\newpage
\section{Motivation}
\textbf{Nowcasting} is the prediction of the present, the near future, and the near past. Nowcasting is important in economics because many important macroeconomic statistics are released with a lengthy delay. For example, the Bureau of Economic Analysis releases quarterly GDP typically two months after the quarter has already ended - a significant delay for any companies or individuals who need the data for planning and forecast models.

This delay is particularly salient during times of high volatility. During the first few months of the COVID-19 pandemic in the United States, many companies attempted to use high-frequency indicators to attempt nowcasts of the state of the macroeconomy. For example, JP Morgan forecasts in March 13 predicted Q2 GDP growth of -3\%; the estimate was revised down to -14\% by March 21, -25\% by March 25, and -40\% by April 10. Yet many of the models were ad-hoc and only able to use a small number of predictive indicators, such as jobless claims or traffic data.

\textbf{Nowcasting is about deciphering key information about the state of the economy before official data is released.} Because of the fundamentally urgent nature of nowcasting, it is important that nowcast models exploit any latest, high-frequency data available. Nowcasts should be able to generate constantly rolling forecasts, updating these numbers in response to any new data releases.




For example, suppose the date is early March, and the variable we want to predict is Q1 GDP. The simplest way to predict Q1 GDP would be to use historical quarterly data from various economic variables. But this data would only be up to Q4 of last year, and would fail to capture the critically important predictive power that could be provided by monthly and daily data released throughout January and February.

Suppose instead, we used monthly data as our predictors of Q1 GDP. Again, we will soon run in to a problem. As an example, imagine that we had imported 20 monthly data series. Suppose 5 of these series ended in December, 10 ended in January, and 5 ended in March. 

\begin{figure}[H]
\includegraphics[scale=.7]{nowcast-time-2}
\centering
\end{figure}

Traditional modeling methods would require us to either throw out variables or throw out months - for example, we could truncate all our data series at January and lose out the information provided by the 5 February data points. Alternatively, we could completely remove the 15 variables with data releasing before March. Both methods are unappealing.

In this paper, we will utilize a methodology that will allow us to use the information from all variables at any dates. This model will give an updated forecast in response to any new data releases. Additionally, the model can be generalized to nowcast any time series variable, not just GDP. The methodology for the model will be described in the next section.
\section{Methodology}

\subsection{Overview}
This paper utilizes a modified version of the two-step dynamic factor model utilized in Giannone et al (2008) and Doz et al (2011). In this model, we use principal components to extract a few fundamental factors describing the overall structure of the economy. These factors are then projected into the future using a vector autoregression; smoothing over missing input data is conducted by using the Kalman filter and smoother. Finally, we aggregate these up to a quarterly level to forecast our quarterly covariates, then use these to calculate our nowcast of GDP.

The model is run every weekday; this documentation will use actual data and estimates from the most recent model run to illustrate the procedure. The final nowcasted output is located in section 3.


\subsection{Data}
We begin by importing monthly data from the St. Louis Federal Reserve Database (FRED). We choose data of at least monthly frequency and with historical data available as of at least 2010. Data are transformed for stationarity as listed below; \textit{dlog} refers to the natural log of the first difference.

<<>>=
	rds$ef$paramsDf %>%
    	dplyr::filter(., dfminput == TRUE) %>%
    	dplyr::select(., fullname, stat) %>%
    	setNames(., c('Variable', 'Stationary Form')) %>%
    	xtable::xtable(., caption = 'Imported Monthly Data', align = 'llc') %>%
    	print(., size = 'scriptsize')
@

We additionally import quarterly data for GDP and its subcomponents.
<<>>=
	rds$ef$paramsDf %>%
    	dplyr::filter(., category == 'Output') %>%
    	dplyr::select(., fullname, stat) %>%
    	setNames(., c('Variable', 'Stationary Form')) %>%
    	xtable::xtable(., caption = 'Imported Quarterly Data', align = 'llc') %>%
    	print(., size = 'scriptsize')
@

Most datasets have already been deseasonalized if necessary by their original source. We deseasonalize the remaining series by using the U.S. Census Bureau's seasonal adjustment package, X13-ARIMA-SEATS. We interface with it by using the \texttt{seasonal} package implementation in R (Sax and Eddelbuettel 2018).

\subsection{Time Periods}
Now we will segment the data by time periods. The imported monthly data will have ragged edges - i.e., some monthly data will be available for later months than others.

We will let $T$ denote the number of dates for which data is available for all data series. $\tau$ will denote the number of dates for which data is available for at least one data series. $T*$ will denote number of dates up to the end-of-quarter month of the $\tau$ date. For example, suppose date $\tau$ occurs on February. The end-of-quarter month, $T^*$, will be March (since Q1 runs through the end of March). 

In other words, data will be indexed by $t = 1, 2, \dots, T, T+1, \dots, \tau, \dots, T^*$, where dates $T + 1$ through $\tau$ are the dates for which only some data are available, and dates $\tau + 1$ through $T^*$ are the dates for which no data is available up to the next quarter-ending month.
\begin{figure}[H]
\includegraphics[scale=.7]{nowcast-time}
\centering
\end{figure}
For our data, we set the dates as follows>
<<>>=
    rds$ef$nc$timeDf %>%
		dplyr::mutate(., date = as.character(date)) %>%
    	dplyr::mutate(., time = ifelse(time == 'Tau', '$\\tau$', paste0('$', time, '$'))) %>%
    	setNames(., c('Date', '$t$')) %>%
    	xtable::xtable(., caption = 'Time Periods', align = 'lcc') %>%
		print(., sanitize.colnames.function = function(x) x, sanitize.text.function = function(x) x)
@

\subsection{Principal Components Analysis}
It is known that a large number of macroeconomic time series are highly correlated; using such covariates as regressors could naturally lead to problems with collinearity and unstable estimates. In addition, it becomes computationally burdensome to analyze data with such a large number of highly correlated variables. Instead, we use principal components analysis (PCA) to shrink our dataset in a way that allows us to retain most of the information in our original data.

Estimation of factors is derived following Stock and Watson (2008). We begin by taking our $T \times N$ data matrix of $N$ monthly covariates, from time 1 through $T$. The matrix, which we denote $X$, is normalized to mean 0 and variance 0 across all columns.

The goal is to minimize the error $E$ below.
\begin{align*}
	X = F  \Lambda ' + E,\\
	\text{where $X$ is the $T \times N$ data matrix,}\\
	\text{$F$ is the $T \times N$ matrix of factors,}\\
	\text{and $\Lambda$ is the weighting matrix.}\\
\end{align*}

Estimation of factors is derived following Stock and Watson (2008).
\begin{align*}
	\widehat{\Lambda} = \text{eigenvectors of } (X'X)\\
	\widehat{F} = X \widehat{\Lambda}
\end{align*}

Once factors are derived, we perform a qualitative check of the factors. Typically the first factor should give us something similar to the growth rate of GDP or aggregate production, but on a monthly basis. The second and third factors may vary but often represent interest rates or consumption. Note that the sign direction of the factors is irrelevant to the modeling process, and they may be switched negated without consequence. 

<<>>=
for (i in 1:length(rds$ef$nc$zPlots)) {
	print(rds$ef$nc$zPlots[[i]])
}
@


Once factors are derived, we select the optimal number of factors to use in predictive regressions. To do so, we use the information criteria from Bai and Ng (2002). Let $R$ refer to the number of factors used. We include alternative specifications of the information criteria from Bai and Ng as a robustness check.
\begin{align*}
	IC(R) = MSE + R \times \frac{N+T}{NT} \times log\left(\frac{NT}{N+T}\right)
\end{align*}

<<fig.cap = 'Factor Selection'>>=
	rds$ef$nc$screePlot
@

<<>>=
	rds$ef$nc$screeDf %>%
    	setNames(., c('Factors (R)', 'Variance Explained', 'Pct of Total Var Explained', 'Cumulative Pct', 'MSE', 'IC1', 'IC2', 'IC3')) %>%
    	xtable::xtable(., caption = 'Factor Selection Process', align = 'lcccccccc') %>%
    	print(., size = 'scriptsize')
@

Choosing the IC-minimizing $R$ lets us choose $R = \Sexpr{rds$ef$nc$bigR}$ factors.



\subsection{Factor VAR}
The next step is to model the transition of the factors over time. To do so, we utilize a vector-autoregressive (VAR) process, following Stock and Watson (2016). As before, $R$ will refer to the total number of factors we extracted in the previous section, and $f^i$ for $i = 1, \dots, R$. will refer to factor $i$.
\begin{align*}
\underbrace{\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}}_{z_t}
=
B
\underbrace{\begin{bmatrix}
	f^1_{t-1}\\
	f^2_{t-1}\\
	\vdots \\
	f^R_{t-1}
\end{bmatrix}}_{z_{t-1}}
+
C
+
\underbrace{\begin{bmatrix}
v^1_t\\
v^2_t\\
\vdots\\
v^R_t
\end{bmatrix}}_{v_t},\\
\text{where $z_t$ is the $R \times 1$ matrix of time $t$ factors,}\\
\text{$B$ is the $R \times R$ coefficient matrix,}\\
\text{$C$ is the $R \times 1$ constant matrix,}\\
\text{and $v_t$ is the $R \times 1$ matrix of errors for time $t$.}
\end{align*}


We wish to estimate the coefficient matrices $B$ and $C$. This can be done via OLS estimation. We first rewrite the data as the standard linear equation,
\begin{align*}
\underbrace{\begin{bmatrix}
f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
f^1_{3} & f^2_{3} & \dots & f^R_{3}\\
\vdots & \vdots & \vdots & \vdots \\
f^1_{T} & f^2_{T} & \dots & f^R_{T}
\end{bmatrix}}_{\Gamma}
=
\underbrace{\begin{bmatrix}
1 & f^1_{1} & f^2_{1} & \dots & f^R_{1}\\
1 & f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & f^1_{T-1} & f^2_{T-1} & \dots & f^R_{T-1}
\end{bmatrix}}_{\Psi}
\underbrace{\begin{bmatrix}
C'\\
B'
\end{bmatrix}}_{\Lambda}
 +
\underbrace{\begin{bmatrix}
v^1_2 & v^2_2 & \dots & v^R_2\\
v^1_3 & v^2_3 & \dots & v^R_3\\
\vdots\\
v^1_T & v^2_T & \dots & v^R_T\\
\end{bmatrix}}_{V},\\
\text{where $\Gamma$ is the $T-1 \times R$ dependent data matrix,}\\
\text{$\Psi$ is the $T-1 \times R+1$ independent data matrix,}\\
\text{$\Lambda$ is the $R+1 \times R$ matrix of coefficient weightings,}\\
\text{and $V$ is the $T-1 \times R$ matrix of residuals.}
\end{align*}
The coefficient matrix $\Lambda$ can be estimated by the standard OLS estimator.
\begin{align*}
\widehat{\Lambda} = (\Psi' \Psi)^{-1} (\Psi'\Gamma)
\end{align*}
It can then be partitioned to calculate $\widehat{B}'$ and $\widehat{C}'$, which can then be transposed to derive our estimates of the original coefficient matrices B and C, $\widehat{B}$ and $\widehat{C}$.

The estimated coefficients in $\widehat{B}$ and $\widehat{C}$ are shown below.
<<>>=
	rds$ef$nc$varCoefDf %>%
    	tidyr::pivot_longer(., -coefname) %>%
    	tidyr::pivot_wider(., names_from = coefname) %>%
    	xtable::xtable(., caption = 'Factor VAR Coefficients') %>%
    	print(.)
@


Finally, we perform a qualitative check of the fitted values and residuals.
<<>>=
for (i in 1:length(rds$ef$nc$varFittedPlots)) {
	print(rds$ef$nc$varFittedPlots[[i]])
}
@


<<fig.cap = 'Factor VAR Residuals'>>=
print(rds$ef$nc$varResidPlot)
@

Goodness-of-fit statistics are shown below.
<<>>=
rds$ef$nc$varGofDf %>%
    xtable::xtable(., caption = 'DFM Goodness of Fit', digits = 8) %>%
    print(.)
@



\subsection{Dynamic Factor Models}
Now let us consider again the monthly covariates which were include in the principal components analysis. We will model these as dynamic factor models (DFMs), i.e. - they are regressed on the factor variables derived from earlier. As before, let $x^i_t$ refer to the time $t$ value for monthly variable $x^i$, where $i = 1, \dots, N$.

The factor models take the following form:
\begin{align*}
\underbrace{\begin{bmatrix}
	x^1_t\\
	x^2_t\\
	\vdots \\
	x^N_t
\end{bmatrix}}_{y_t}
=
A
\underbrace{\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}}_{z_t}
+
D 
+
\underbrace{\begin{bmatrix}
	w^1_t\\
	w^2_t\\
	\vdots\\
	w^N_t
\end{bmatrix}}_{w_t},\\
\text{where $y_t$ is the $N \times 1$ vector of monthly variables at time $t$,}\\
\text{$A$ is the $N \times R$ coefficient matrix,}\\
\text{$z_t$ is the $R \times 1$ vector of factors at time $t$,}\\
\text{$D$ is the $N \times 1$ constant matrix,}\\
\text{and $w_t$ is the $N \times 1$ vector of errors at time $t$.}\\
\end{align*}
We wish to estimate the coefficient matrices $A$ and $D$. As before, we can do this by estimating this as an OLS equation, writing the data matrices as follows

\begin{equation}
\underbrace{\begin{bmatrix}
x^1_{2} & x^2_{2} & \dots & x^N_{2}\\
x^1_{3} & x^2_{3} & \dots & x^N_{3}\\
\vdots & \vdots & \vdots & \vdots \\
x^1_{T} & x^2_{T} & \dots & x^N_{T}\\
\end{bmatrix}}_{\Phi}
=
\underbrace{\begin{bmatrix}
1 & f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
1 & f^1_{3} & f^2_{3} & \dots & f^R_{3}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & f^1_{T} & f^2_{T} & \dots & f^R_{T}\\
\end{bmatrix}}_{\Omega}
\begin{bmatrix}
D'\\
A'
\end{bmatrix}
 +
\underbrace{\begin{bmatrix}
w^1_2 & w^2_2 & \dots & w^R_2\\
w^1_3 & w^2_3 & \dots & w^R_3\\
\vdots\\
w^1_T & w^2_T & \dots & w^R_T\\
\end{bmatrix}}_{W}
\end{equation}

As before we can estimate $A$ and $D$ with the standard OLS estimator.
\begin{align*}
\begin{bmatrix}
	\widehat{A}\\
	\widehat{D}
\end{bmatrix} =
(\Omega' \Omega)^{-1} (\Omega'\Phi)
\end{align*}

Estimated coefficients for $\widehat{A}$ and $\widehat{D}$ are below.
<<>>= 
rds$ef$nc$dfmCoefDf %>% tidyr::pivot_longer(., -coefname) %>% tidyr::pivot_wider(., names_from = coefname) %>%
    	xtable::xtable(., caption = 'Estimated DFM Coefficients') %>%
    	print(.)
@

We perform a qualitative check of the in-sample fit, as well as a quantitative review of the goodness-of-fit. The graphs of the fitted plots are located in the appendix.
<<>>=
	rds$ef$nc$dfmGofDf %>%
    	xtable::xtable(., caption = 'DFM Goodness of Fit', digits = 8) %>%
    	print(.)
@





\subsection{State-Space Setup}
Now, combining our equations for the DFM and the VAR, we have the below system.
\begin{align*}
z_t = B z_{t-1} + Cx + v_t\\
y_t = A z_t + w_t
\end{align*}

This system is now fully specified and in state-space form. The first equation is our state (or transition) equation. The second equation is our measurement equation. 

We use our estimated values $B$, $C$, and $A$ calculated in our previous two sections. To run the Kalman Filter, we will want to create the actual data matrices for $z_t$ and $y_t$. $z_t$ can be constructed as before, using data for factors from time 1 through $T$. However, unlike in the previous two sections, we will want to create $y_t$ matrices not for just time periods 1 through $T$, but now for time periods $1$ through $\tau$. Elements in $y_t$ may be set to any value for missing observations; the process of Kalmam filtration will render this choice irrelevant.

Specifically, we construct the matrices below.
\begin{align*}
z_t
=
\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}, \forall t \in 1, \dots, T\\
y_t
=
\begin{bmatrix}
	\text{$x^1_{t}$ if available, otherwise 0}\\
	\text{$x^2_{t}$ if available, otherwise 0}\\
	\vdots \\
	\text{$x^N_{t}$ if available, otherwise 0}\\
\end{bmatrix}, \forall t \in 1, \dots, \tau
\end{align*}

For Kalman filtration, we also require an assumed distribution on $v_t$ and $w_t$. We assume that $v_t$ is distributed normally with mean 0 and constant diagonal covariance matrix denoted $Q$, with diagonal entries calculated by taking the average squared values of the residuals of the VAR.

We also assume $w_t$ is distributed normally with mean $0$. However, we no longer specify the covariance matrix as constant, but as the time-dependent matrices $R_t$. For $t \in 1, \dots, T+1$, we let $R_t$ be a diagonal covariance matrix with diagonal entries calcualted by taking the average squared values of the residuals of the DFM. For $t \in T+1, \dots, \tau$, we let the diagonal elements of $R_t$ be equal to infinity if the corresponding element of $y_t$ is missing for that time period; and equal to the average squared value of the residual if otherwise.
\begin{align*}
	v_t \sim \mathcal{N}(0, Q)\\
	w_t \sim \mathcal{N}(0, R_t)
\end{align*}

We begin with the unconditional mean of $\widehat{Z}_{0|-1} = 0$ and unconditional variance of VAR $\Sigma_{0|-1} = 0$.

\subsection{Kalman Filtration}
Now that our state-space model is fully specified, we can begin the Kalman filter recursions.
\begin{align*}
	z_t = B z_{t-1} + Cx + v_t\\
	y_t = A z_t + Dx + w_t\\
	v_t \sim \mathcal{N}(0, Q)\\
	w_t \sim \mathcal{N}(0, R_t)
\end{align*}

To solve this programmatically, we will need the previously estimated matrices $A$, $B$, $C$, and $D$; the matrices $z_t$ from 1 through $T$; the matrices $y_t$ from 1 through $\tau$; the covariance matrix $Q$; and finally, the covariance matrices $R_t$ from 1 through $\tau$.

We initialize the Kalman filter with the following standard assumptions. 
\begin{align*}
	\vv{z}_{0|0} = 0\\
	\vv{CovZ} = 0
\end{align*}

Now for $t = 1, \dots, \tau$, we iterate through the Kalman filter recursions and iteratively calculate the values below.
\begin{align*}
	\vv{z}_{t|t-1} &= B \vv{z}_{t-1|t-1} + C\\
	\vv{CovZ}_{t|t-1} &= B \vv{CovZ}_{t-1|t-1} + Q\\
	\vv{y}_{t|t-1} &= A \vv{z}_{t|t-1} + D\\
	\vv{CovY}_{t|t-1} &= A \vv{CovZ}_{t|t-1} A' + R_t\\
	P_t &= \vv{CovZ}_{t|t-1} A' \vv{CovY}^{-1}_{t|t-1}\\
	\vv{z}_{t|t} &= \vv{z}_{t|t-1} + P_t (\vv{y}_t - \vv{y}_{t|t-1})\\
	\vv{CovZ}_{t|t} &= \vv{CovZ}_{t|t-1} - P_t (\vv{CovY}_{t|t-1}) P'_t
\end{align*}
Note that the during recursions $T + 1, \dots \tau$, the infinite values in the $R_t$ matrix will cause infinite values in the $\vv{CovY}_{t|t-1}$ matrix. This may prevent standard computational methods from computing the inverse of the matrix needed in the step for calculation of $\vv{CovZ}_{t|t}$. Alternative methods, such as a Cholesky decomposition before inversion, can be used to subvert this problem.

The Kalman filter allows us to recover all the time $t$ conditional state matrices $z_{t|t}$ that have been adjusted for information from the monthly datasets. However, of more interest to us is the value of the state matrices when conditioned on all data available at time $\tau$, $z_{t|\tau}$. This can be recovered by using the Kalman smoother.

Recursively iterating over $t = \tau - 1, \dots, 1$, we calculate the following values. 
\begin{align*}
	S_{t} &= \vv{CovZ}_{t|t} B' \vv{CovZ}^{-1}_{t + 1|t}\\
	\vv{z}_{t|\tau} &= \vv{z}_{t|t} + S_t (\vv{z}_{t+1|\tau} - \vv{z}_{t + 1|t})\\
	\vv{CovZ}_{t|\tau} &= \vv{CovZ}_{t|t} - S_t(\vv{CovZ}_{t + 1|t} - \vv{CovZ}_{t + 1|\tau})S'_t
\end{align*}
These values $\vv{z}_{t|\tau}$ will serve as our estimates of the state variables (i.e., the PCA factors) from time 1 through $\tau$.

Finally, we want to forecast the the state vector $z_{t|\tau}$ for $t = \tau + 1, \dots, T^*$. This can be done through the typical Kalman filter forecasting step.

Recursively iterating over $t = \tau + 1, \dots, T^*$, we calculate the following values.
\begin{align*}
	\vv{z}_{t|\tau} &= B \vv{z}_{t-1|\tau} + C\\
	\vv{CovZ}_{t|\tau} &= B \vv{CovZ}_{t-1|\tau} B'+ Q\\
	\vv{y}_{t|\tau} &= A \vv{z}_{t|\tau} + D\\
	\vv{CovY}_{t|\tau} &= A \vv{CovZ}_{t|\tau} A' + R_0
\end{align*}
Combining the calculations for $\vv{z}_{t|\tau}$ with the ones derived from the Kalman smoother, we will now be able to obtain the full time series for the factors from time 1 through time $T^*$.

<<>>=
    for (i in 1:length(rds$ef$nc$kfPlots)) {
    	print(rds$ef$nc$kfPlots[[i]])
    }
@

% \subsection{Nowcasting Monthly Covariates}
% Now that we have our Kalman-smoothed factors from time $1$ through time $T^*$, we will be able to use these as covariates to model any monthly time series that we have. 

WORK IN PROGRESS


\subsection{Predicting Quarterly Variables}
Now that we have our Kalman-smoothed and forecasted factors from time $1$ through time $T^*$, we will be able to use these as covariates to model any monthly time series that we have.

We begin by aggregating these monthly factors into quarterly data by taking a simple monthly average for each factor over each quarter.

In this section, we will use these now-quarterly factors to forecast our quarterly subcomponents of GDP. As discussed in the data import section, these have been transformed for stationarity, typically by taking a log-difference.
<<>>=
rds$ef$paramsDf %>%
    	dplyr::filter(., category == 'Output' & nowcast == 'dfm.q') %>%
    	dplyr::transmute(., Variable = fullname) %>%
    	xtable::xtable(., caption = 'Quarterly Data Covariates') %>%
    	print(., size = 'scriptsize')
@
We will notate each of these gdp subcomponents as $y^i$ and $M$ as the total number of covariates, so that $i = 1, \dots, M$.

Note that many higher-level components of GDP (including GDP itself) are \textit{not} forecasted directly in this step; these will be forecasted later by aggregating their subcomponents.

Now we will specify that these GDP subcomponents follow a DFM-AR(1) model; i.e., they will be functions of the monthly-aggregated factors as well as the first lag of themselves.
\begin{align*}
	y^i_t
	=
	\beta
	\begin{bmatrix}
	1 \\
	y^i_{t-1}\\
	f^1_t \\
	\vdots\\
	f^R_t
	\end{bmatrix}
	+
	e_t
\end{align*}
The $\beta$ coefficients can be estimated with a typical OLS process, where the training data is constituted of the $y^i_t$ quarterly covariates, the lagged quarterly covariates $y^i_{t-1}$, and the quarterly-aggregated factor variables. The data is cut off at the date for which the any data on the quarterly covariates are missing.

After estimation, we then use the same model to forecast forward the $y^i_t$ quarterly, up through time $\tau$. 
The forecasted results are as follows.
<<>>=
rds$ef$ncpred0$q$stat %>%
    	tidyr::pivot_longer(., -obsDate, names_to = 'varname') %>%
    	dplyr::inner_join(., filter(rds$ef$paramsDf, category == 'Output')[, c('varname', 'fullname')], by = 'varname') %>%
    	dplyr::mutate(., obsDate = paste0(year(obsDate), 'Q', quarter(obsDate))) %>%
    	dplyr::select(., -varname) %>%
    	tidyr::pivot_wider(., values_from = value, names_from = obsDate) %>%
    	dplyr::rename(., 'Variable' = fullname) %>%
    	xtable::xtable(., caption = 'DFM-AR(1) Forecasted GDP Subcomponents') %>%
    	print(., size = 'scriptsize')
@

We can then backtransform the data so that the units are in base values. After backtransformation, we are ready to aggregate these up to higher-level GDP components. In particular, we calculate the variables below.
<<>>=
rds$ef$paramsDf %>%
    	dplyr::filter(., category == 'Output' & nowcast == 'calc') %>%
    	dplyr::transmute(., Variable = fullname) %>%
    	xtable::xtable(., caption = 'Summable Quarterly Data Covariates') %>%
    	print(., size = 'scriptsize')
@
These are calculated using the standard GDP aggregation equations, e.g., net exports = exports - imports, and so on.
Finally, we convert these into annualized percentage change, as this is the standard format in which GDP subcomponents are reported in.

\section{Results}
Our final nowcasts of GDP and its subcomponents are below. All units are reported in terms of annualized percentage change (seasonally adjusted) except for change in private inventories and net exports, which are reported in terms of billions of real 2012 dollars.
<<>>=
rds$ef$ncpred$q$display %>%
	tidyr::pivot_longer(., -obsDate, names_to = 'varname') %>%
	dplyr::inner_join(filter(rds$ef$paramsDf, category == 'Output')[, c('varname', 'fullname')], ., by = 'varname') %>%
	# Add tabs
    rowwise(.) %>%
    dplyr::mutate(., fullname = paste0(paste0(rep(' ', str_count(fullname, ':')), collapse = ''), fullname)) %>%
    dplyr::mutate(., fullname1 = paste0('\\hspace{',str_count(fullname, ':') * 8, 'mm} ')) %>%
    dplyr::mutate(., fullname2 = sub('.*\\:', '', fullname)) %>%
    dplyr::mutate(., fullname2 = str_replace(fullname2, coll('&'), '\\&')) %>%
    dplyr::mutate(., fullname2 = ifelse(str_count(fullname, ':') == 0, paste0('\\textbf{', fullname2, '}'), fullname2)) %>%
    dplyr::mutate(., fullname = paste0(fullname1, fullname2)) %>%
    dplyr::select(., -fullname1, -fullname2) %>%
    dplyr::ungroup(.) %>%
    # End
	dplyr::mutate(., obsDate = paste0(year(obsDate), 'Q', quarter(obsDate))) %>%
	dplyr::select(., -varname) %>%
	tidyr::pivot_wider(., values_from = value, names_from = obsDate) %>%
	dplyr::rename(., 'Variable' = fullname) %>%
	xtable::xtable(., caption = 'Nowcasts for GDP and Subcomponents (Annualized Percent Change)') %>%
	print(., size = '\\fontsize{11pt}{13pt}\\selectfont', sanitize.text.function = function(x) x)
@




\appendix
\appendixpage
\addappheadtotoc

\section{DFM Fitted Plots}
<<>>=
for (i in 1:length(rds$ef$nc$dfmFittedPlots)) {
	print(rds$ef$nc$dfmFittedPlots[[i]])
}

@


\end{document}