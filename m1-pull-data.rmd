# Set Constants
```{r purl = TRUE}
DIR = 'D:/Onedrive/__Projects/econforecasting'
DL_DIR = 'D:/Onedrive/__Projects/econforecasting/tmp'
PACKAGE_DIR = 'D:/Onedrive/__Projects/econforecasting/r-package' # Path to package with helper functions
INPUT_DIR = 'D:/Onedrive/__Projects/econforecasting/model-inputs' # Path to directory with constants.r (SQL DB info, SFTP info, etc.)
OUTPUT_DIR = 'D:/Onedrive/__Projects/econforecasting/model-outputs'
VINTAGE_DATE = Sys.Date()
```


# Initialize
```{r}
library(tidyverse)
library(data.table)
library(devtools)
library(jsonlite)
library(lubridate)
library(httr)
library(rvest)
library(DBI)
devtools::load_all(path = PACKAGE_DIR)
devtools::document(PACKAGE_DIR)

# Set working directory
setwd(DIR)

# Create temp directory if doesn't exist
if (dir.exists(DL_DIR)) unlink(DL_DIR, recursive = TRUE)
dir.create(DL_DIR, recursive = TRUE)

# Read constants
source(file.path(INPUT_DIR, 'constants.r'))

# Set top level variables - params, history, model
p = list(
	variablesDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'all-variables'),
	forecastsDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'ext-forecasts'),
	releasesDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'releases'),
	VINTAGE_DATE = VINTAGE_DATE
	) %>%
	c(., list(variables = purrr::transpose(.$variablesDf, .names = .$variablesDf$varname)))

h = list()
m = list()


# Check that each variable has a correct release
dplyr::left_join(p$variablesDf, p$releasesDf, by = 'relkey') %>%
	.$relname %>%
	keep(., ~ is.na(.)) %>%
	{if(length(.) != 0) stop('Error')}
```


# Get Historical Data

## FRED Releases
```{r}
local({
	
	releaseDf =
		p$variablesDf %>%
		dplyr::group_by(., relkey) %>%
		dplyr::summarize(., n_varnames = n(), varnames = jsonlite::toJSON(fullname), .groups = 'drop') %>%
		dplyr::left_join(
			.,
			p$variablesDf %>%
				dplyr::filter(., nc_dfm_input == 1) %>%
				dplyr::group_by(., relkey) %>%
				dplyr::summarize(., n_dfm_varnames = n(), dfm_varnames = jsonlite::toJSON(fullname), .groups = 'drop'),
			by = 'relkey'
		) %>%
		dplyr::left_join(
			.,
			p$releasesDf,
			by = 'relkey'
		) %>%
		# Now create a column of included releaseDates
		dplyr::left_join(
			.,
			purrr::map_dfr(purrr::transpose(dplyr::filter(., relsc == 'fred')), function(x)
				httr::RETRY(
					'GET', 
					str_glue(
						'https://api.stlouisfed.org/fred/release/dates?',
						'release_id={x$relsckey}&realtime_start=2020-01-01',
						'&include_release_dates_with_no_data=true&api_key={CONST$FRED_API_KEY}&file_type=json'
						 ),
					times = 10
					) %>%
			        httr::content(., as = 'parsed') %>%
					.$release_dates %>%
					sapply(., function(y) y$date) %>%
					tibble(relkey = x$relkey, reldates = .)
				) %>%
				dplyr::group_by(., relkey) %>%
				dplyr::summarize(., reldates = jsonlite::toJSON(reldates), .groups = 'drop')
				,
			by = 'relkey'
		)
	
	p$releasesDf <<- releaseDf
})
```


## FRED
```{r}
local({
	
	fredRes =
		p$variables %>%
		purrr::keep(., ~ .$source == 'fred') %>%
		lapply(., function(x) {
			
			message('Getting data ... ', x$varname)

			# Get series data
			dataDf =
				econforecasting::getDataFred(x$sckey, CONST$FRED_API_KEY, .freq = x$freq, .returnVintages = TRUE, .vintageDate = VINTAGE_DATE) %>%
			  	dplyr::filter(., vintageDate <= VINTAGE_DATE) %>%
			  	dplyr::filter(., vintageDate == max(vintageDate)) %>%
			  	dplyr::select(., -vintageDate) %>%
				dplyr::transmute(., date = obsDate, value) %>%
				dplyr::filter(., date >= as.Date('2010-01-01'))

			list(dataDf = dataDf)
			})
	
	
	for (varname in names(fredRes)) {
		p$variables[[varname]]$rawData <<- fredRes[[varname]]$dataDf
	}
})
```

## Yahoo Finance
```{r}
local({
	
	res =
		p$variables %>%
    	purrr::keep(., ~ .$source == 'yahoo') %>%
		lapply(., function(x) {
			url =
				paste0(
					'https://query1.finance.yahoo.com/v7/finance/download/', x$sckey,
					'?period1=', '946598400', # 12/30/1999
					'&period2=', as.numeric(as.POSIXct(Sys.Date() + lubridate::days(1))),
					'&interval=1d',
					'&events=history&includeAdjustedClose=true'
				)
			data.table::fread(url, showProgress = FALSE) %>%
				.[, c('Date', 'Adj Close')]	%>%
				setnames(., new = c('date', 'value')) %>%
				as_tibble(.) %>%
				dplyr::filter(., date <= p$VINTAGE_DATE) %>%
				return(.)
		})
	
	for (varname in names(res)) {
		p$variables[[varname]]$rawData <<- res[[varname]]
	}
})
```



# Aggregate Frequencies

## Move to Hist Object
```{r}
local({
	
	 res =
	 	p$variables %>%
	 	purrr::keep(., ~ is_tibble(.$rawData)) %>%
	 	lapply(., function(x)
	 		list(x$rawData) %>%
	 			setNames(., x$freq)
	 		)
	 
	for (varname in names(res)) {
		p$variables[[varname]]$h$base <<- res[[varname]]
	}
})
```

## Monthly Agg
```{r}
local({
	
    res =
        # Get all daily/weekly varnames with pre-existing data
        p$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w') && is_tibble(.$rawData)) %>%
        # Add monthly values for current month
        lapply(., function(x) {
        	x$rawData %>%
        		dplyr::mutate(., date = as.Date(paste0(year(date), '-', month(date), '-01'))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop') %>%
        		dplyr::mutate(., freq = 'm') %>%
        		{
        			# Keep last-month aggregation despite possible missing data if set in inputs.xlsx
        			if (x$append_eom_with_currentval == 1) .
        			else dplyr::filter(., date != max(date))
        		}
        	})
    
	for (varname in names(res)) {
		p$variables[[varname]]$h$base$m <<- res[[varname]]
	}
})
```

## Quarterly Aggregation
```{r}
local({
	
    res =
        p$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w', 'm') && is_tibble(.$rawData)) %>%
        lapply(., function(x) {
        	message(x$varname)
        	x$h$base$m %>%
        		dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
        		# Only keep if all 3 monthly data exists (or imputed by previous chunk)
        		dplyr::filter(., n == 3) %>%
        		dplyr::select(., -n)
        	})
    
	for (varname in names(res)) {
		p$variables[[varname]]$h$base$q <<- res[[varname]]
	}
    
})
```



# Add Calculated Variables

## Trailing Inflation
```{r}
local({
	
    mDf =
    	p$variables$cpi$h$base$m %>%
    	dplyr::mutate(., value = (value/dplyr::lag(value, 13) - 1) * 100) %>%
    	na.omit(.)
    
    qDf =
    	mDf %>%
    	dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
        dplyr::group_by(., date) %>%
        dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
        # Only keep if all 3 monthly data exists (or imputed by previous chunk)
        dplyr::filter(., n == 3) %>%
        dplyr::select(., -n)
    
    p$variables$inf$h$base$m <<- mDf
    p$variables$inf$h$base$q <<- qDf
})
```


## DNS Model - Interest Rates
Let tyield = ffr + dns_curve(ttm)
Exogenously choose ffr and tyield_10y_3m (negative of 10year - 3month spread; 3-month driven heavily by ffr)
```{r}
local({
  
    # Create tibble mapping tyield_3m to 3, tyield_1y to 12, etc.
    yieldCurveNamesMap =
        p$variables %>% 
        map_chr(., ~.$varname) %>%
        unique(.) %>%
        purrr::keep(., ~ str_sub(., 1, 1) == 't' & str_length(.) == 4) %>%
        tibble(varname = .) %>%
        dplyr::mutate(., ttm = as.numeric(str_sub(varname, 2, 3)) * ifelse(str_sub(varname, 4, 4) == 'y', 12, 1))
  
    # Create training dataset from SPREAD from ffr - fitted on last 3 months
    trainDf =
        purrr::map_dfr(yieldCurveNamesMap$varname, function(x) p$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)) %>%
        dplyr::select(., -freq) %>%
        dplyr::filter(., date >= add_with_rollback(p$VINTAGE_DATE, months(-3))) %>%
        dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
        dplyr::left_join(., dplyr::transmute(p$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
        dplyr::mutate(., value = value - ffr) %>%
        dplyr::select(., -ffr)
  
    # @param df: (tibble) A tibble continuing columns obsDate, value, and ttm
    # @param returnAll: (boolean) FALSE by default.
    # If FALSE, will return only the MAPE (useful for optimization).
    # Otherwise, will return a tibble containing fitted values, residuals, and the beta coefficients.
    getDnsFit = function(df, lambda, returnAll = FALSE) {
        df %>%
        dplyr::mutate(
        .,
        f1 = 1,
        f2 = (1 - exp(-1 * lambda * ttm))/(lambda * ttm),
        f3 = f2 - exp(-1 * lambda * ttm)
        ) %>%
        dplyr::group_by(date) %>%
        dplyr::group_split(.) %>%
        lapply(., function(x) {
        reg = lm(value ~ f1 + f2 + f3 - 1, data = x)
        dplyr::bind_cols(x, fitted = fitted(reg)) %>%
        dplyr::mutate(., b1 = coef(reg)[['f1']], b2 = coef(reg)[['f2']], b3 = coef(reg)[['f3']]) %>%
        dplyr::mutate(., resid = value - fitted)
        }) %>%
        dplyr::bind_rows(.) %>%
        {
        if (returnAll == FALSE) dplyr::summarise(., mse = mean(abs(resid))) %>% .$mse
        else .
        } %>%
        return(.)
    }

    # Find MSE-minimizing lambda value
    optimLambda =
        optimize(
            getDnsFit,
            df = trainDf,
            returnAll = FALSE,
            interval = c(-1, 1),
            maximum = FALSE
            )$minimum
  
    mDf =
        purrr::map_dfr(yieldCurveNamesMap$varname, function(x) p$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)) %>%
        dplyr::select(., -freq) %>%
        dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
        dplyr::left_join(., dplyr::transmute(p$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
        dplyr::mutate(., value = value - ffr) %>%
        dplyr::select(., -ffr) %>%
        getDnsFit(., lambda = optimLambda, returnAll = TRUE) %>%
        dplyr::group_by(., date) %>%
        dplyr::summarize(., tdns1 = unique(b1), tdns2 = unique(b2), tdns3 = unique(b3)) %>%
        tidyr::pivot_longer(., -date, names_to = 'varname')

    mDfs = mDf %>% split(., as.factor(.$varname)) %>% lapply(., function(x) x %>% dplyr::select(., -varname))

    qDfs = mDfs %>% lapply(., function(x) monthlyDfToQuarterlyDf(x))
  
    # Store DNS coefficients
    dnsCoefs =
        getDnsFit(df = trainDf, optimLambda, returnAll = TRUE) %>%
        dplyr::filter(., date == max(date)) %>%
        dplyr::select(., b1, b2, b3) %>%
        head(., 1) %>%
        as.list(.)

    dnsFitChart =
        getDnsFit(df = trainDf, optimLambda, returnAll = TRUE) %>%
        dplyr::filter(., date == max(date)) %>%
        dplyr::arrange(., ttm) %>%
        ggplot(.) +
        geom_point(aes(x = ttm, y = value)) +
        geom_line(aes(x = ttm, y = fitted))


    for (varname in names(mDfs)) {
        p$variables[[varname]]$h$base$m <<- mDfs[[varname]]
        p$variables[[varname]]$h$base$q <<- qDfs[[varname]]
    }

    dnsFitChart
    
    m$dnsLambda <<- optimLambda
    m$dnsCoefs <<- dnsCoefs
    m$dnsFitChart <<- dnsFitChart
    m$dnsYieldCurveNamesMap <<- yieldCurveNamesMap
})
```

## Other Interest Rate Spreads
```{r}
local({
    
    resDfs =
        tribble(
            ~varname, ~var1, ~var2,
            'mort30ymort15yspread', 'mort30y', 'mort15y',
            'mort15yt10yspread', 'mort15y', 't10y'
            ) %>%
        purrr::transpose(., .names = .$varname) %>%
        lapply(., function(x) {
            m =
                dplyr::inner_join(
                    p$variables[[x$var1]]$h$base$m,
                    p$variables[[x$var2]]$h$base$m %>% dplyr::rename(., v2 = value),
                    by = 'date'
                ) %>%
                dplyr::transmute(., date, value = value - v2)
            q = monthlyDfToQuarterlyDf(m)
            list(m = m, q = q)
            })
    
  for (varname in names(resDfs)) {
	p$variables[[varname]]$h$base <<- resDfs[[varname]]
  }
})
```



# Transformations

## Deseasonalize
```{r}
local({
	# seasDf =
	# 	p$h$sourceDf %>%
	# 	dplyr::filter(., varname == 'hpils') %>%
	# 	dplyr::mutate(
	# 		.,
	# 		seas = 
	# 		{ts(.$value, start = c(year(.$date[1]), month(.$date[1])), freq = 12)} %>%
	# 		seasonal::seas(.) %>%
	# 		predict(.)
	# 		) %>%
	# 	dplyr::select(., -value)
	# 
	# df =
	# 	dplyr::left_join(p$h$sourceDf, seasDf, by = c('date', 'varname', 'freq')) %>%
	# 	dplyr::mutate(., value = ifelse(is.na(seas), value, seas)) %>%
	# 	dplyr::select(., -seas)
	# 
	# p$h$seasDf <<- df
})
```

## Stationarity
```{r}
local({

	resDfs =
		p$variables %>%
		lapply(., function(x) {
			
			message(x$varname)
			
			lapply(c('st', 'd1', 'd2') %>% setNames(., .), function(form) {
			
				if (x[[form]] == 'none') return(NULL)
				# Now iterate through sub-frequencies
				lapply(x$h$base, function(df)
					df %>%
						dplyr::arrange(., date) %>%
						dplyr::mutate(
							.,
							value = {
								if (x[[form]] == 'base') value
								else if (x[[form]] == 'dlog') dlog(value)
								else if (x[[form]] == 'diff1') diff1(value)
								else if (x[[form]] == 'pchg') pchg(value)
								else if (x[[form]] == 'apchg') apchg(value)
								else stop ('Error')
								}
							) %>%
						na.omit(.)
					)
				}) %>%
				purrr::compact(.)
			})

    for (varname in names(resDfs)) {
		for (form in names(resDfs[[varname]])) {
			p$variables[[varname]]$h[[form]] <<- resDfs[[varname]][[form]]
		}
	}
})
```



# Aggregate

## Flat
```{r}
local({
	
	flatDf =
		purrr::imap_dfr(p$variables, function(x, varname)
			purrr::imap_dfr(x$h, function(y, form)
				purrr::imap_dfr(y, function(z, freq)
					z %>% dplyr::mutate(., freq = freq, form = form, varname = varname)
					)
				)
			) %>%
		dplyr::filter(., freq %in% c('m', 'q'))

	h$flatDf <<- flatDf
})
```


## Create monthly/quarterly matrixes
```{r}
local({
	
	wide =
		h$flatDf %>%
		as.data.table(.) %>%
		split(., by = 'form') %>%
		lapply(., function(x)
			split(x, by = 'freq') %>%
				lapply(., function(y)
					as_tibble(y) %>%
						dplyr::select(., -freq, -form) %>%
						tidyr::pivot_wider(., names_from = varname) %>%
						dplyr::arrange(., date)
					)
			)

	h$base <<- wide$base
	h$st <<- wide$st
	h$d1 <<- wide$d1
	h$d2 <<- wide$d2
})
```

# External

## Atlanta Fed
```{r}
local({
  
  ##### GDP #####
  paramsDf =
	tribble(
      ~ varname, ~ fredId, 
      'gdp', 'GDPNOW',
      'pce', 'PCECONTRIBNOW'
    )
  
  # GDPNow
  df =
  	lapply(paramsDf %>% purrr::transpose(.), function(x)
      getDataFred(x$fredId, CONST$FRED_API_KEY, .returnVintages = TRUE) %>%
        dplyr::filter(., obsDate >= .$vintageDate - months(3)) %>%
        dplyr::transmute(., fcname = 'atl', varname = x$varname, form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)
      ) %>%
    dplyr::bind_rows(.)

	m$ext$sources$atl <<- df
})
```

## St. Louis Fed
```{r}
local({
 
	df =
		getDataFred('STLENI', CONST$FRED_API_KEY, .returnVintages = TRUE) %>%
		dplyr::filter(., obsDate >= .$vintageDate - months(3)) %>%
		dplyr::transmute(., fcname = 'stl', varname = 'gdp', form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)

    
	m$ext$sources$stl <<- df
})
```

## New York Fed
```{r}
local({
  
    file = file.path(DL_DIR, 'nyf.xlsx')
    httr::GET(
        'https://www.newyorkfed.org/medialibrary/media/research/policy/nowcast/new-york-fed-staff-nowcast_data_2002-present.xlsx?la=en',
        httr::write_disk(file, overwrite = TRUE)
        )
  
    df =
        readxl::read_excel(file, sheet = 'Forecasts By Quarter', skip = 13) %>%
        dplyr::rename(., vintageDate = 1) %>%
        dplyr::mutate(., vintageDate = as.Date(vintageDate)) %>%
        tidyr::pivot_longer(., -vintageDate, names_to = 'obsDate', values_to = 'value') %>%
        na.omit(.) %>%
        dplyr::mutate(., obsDate = econforecasting::strdateToDate(obsDate)) %>%
        dplyr::transmute(., fcname = 'nyf', varname = 'gdp', form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)

  m$ext$sources$nyf <<- df
})
```

## Philadelphia Fed
```{r}
local({
  
    # Scrape vintage dates
    vintageDf =
        httr::GET('https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/spf-release-dates.txt?la=en&hash=B0031909EE9FFE77B26E57AC5FB39899') %>%
        httr::content(., as = 'text', encoding = 'UTF-8') %>%
        str_sub(
            .,
            str_locate(., coll('1990 Q2'))[1], str_locate(., coll('*The 1990Q2'))[1] - 1
            ) %>%
        readr::read_table(., col_names = FALSE) %>%
        tidyr::fill(., X1, .direction = 'down') %>%
        na.omit(.) %>%
        dplyr::transmute(
            .,
            releaseDate = econforecasting::strdateToDate(paste0(X1, X2)),
            vintageDate = lubridate::mdy(str_replace_all(str_extract(X3, "[^\\s]+"), '[*]', ''))
        ) %>%
        # Don't include first date - weirdly has same vintage date as second date
        dplyr::filter(., releaseDate >= as.Date('2000-01-01'))
        
    paramsDf =
        tribble(
          ~ varname, ~ spfname, ~ method,
          'gdp', 'RGDP', 'growth',
          'pce', 'RCONSUM', 'growth',
          'ue', 'UNEMP', 'level',
          't03m', 'TBILL', 'level',
          't10y', 'TBOND', 'level',
          'houst', 'HOUSING', 'level',
          'inf', 'CORECPI', 'level'
          )
  
  
	df =
		lapply(c('level', 'growth'), function(m) {
		    file = file.path(DL_DIR, paste0('spf-', m, '.xlsx'))
	        httr::GET(
	            paste0(
	                'https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/median', m, '.xlsx?la=en'),
	                httr::write_disk(file, overwrite = TRUE)
	            )
	        
	        lapply(paramsDf %>% dplyr::filter(., method == m) %>% purrr::transpose(.), function(x) {
	            readxl::read_excel(file, na = '#N/A', sheet = x$spfname) %>%
	            dplyr::select(
	            .,
	            c('YEAR', 'QUARTER', {
	              if (m == 'level') paste0(x$spfname, 2:6) else paste0('d', str_to_lower(x$spfname), 2:6)
	              })
	            ) %>%
	            dplyr::mutate(., releaseDate = econforecasting::strdateToDate(paste0(YEAR, 'Q', QUARTER))) %>%
	            dplyr::select(., -YEAR, -QUARTER) %>%
	            tidyr::pivot_longer(., -releaseDate, names_to = 'fcPeriods') %>%
	            dplyr::mutate(., fcPeriods = as.numeric(str_sub(fcPeriods, -1)) - 2) %>%
	            dplyr::mutate(., obsDate = add_with_rollback(releaseDate, months(fcPeriods * 3))) %>%
	            na.omit(.) %>%
	            dplyr::inner_join(., vintageDf, by = 'releaseDate') %>%
	            dplyr::transmute(., fcname = 'spf', varname = x$varname, form = 'd1', vdate = vintageDate, date = obsDate, value)
	            }) %>%
	            dplyr::bind_rows(.) %>%
	            return(.)
	        
	        }) %>%
	        dplyr::bind_rows(.)
        
	m$ext$sources$spf <<- df
})
```

## WSJ Economic Survey
WSJ Survey Updated to Quarterly - see https://www.wsj.com/amp/articles/economic-forecasting-survey-archive-11617814998
```{r}
local({
    
    orgsDf =
        tibble(
          fcname = c('wsj', 'fnm', 'wfc', 'gsu', 'spg', 'ucl', 'gsc'),
          fcfullname = c('WSJ Consensus', 'Fannie Mae', 'Wells Fargo & Co.', 'Georgia State University', 'S&P Global Ratings', 'UCLA Anderson Forecast', 'Goldman, Sachs & Co.')
        )
    
    filePaths =
        seq(as.Date('2021-01-01'), to = add_with_rollback(p$VINTAGE_DATE, months(0)), by = '3 months') %>%
        purrr::map(., function(x)
            list(
                date = x,
                file = paste0('wsjecon', str_sub(x, 6, 7), str_sub(x, 3, 4), '.xls')
                )
            )
    
    df =
        lapply(filePaths, function(x) {
            message(x$date)
            dest = file.path(DL_DIR, 'wsj.xls')

            httr::http_error('https://google.com')
            download.file(
                paste0('https://online.wsj.com/public/resources/documents/', x$file),
                destfile = dest,
                mode = 'wb',
                quiet = TRUE
                )
            
            # Read first two lines to parse column names
            xlDf = suppressMessages(readxl::read_excel(dest, col_names = FALSE, .name_repair = 'unique'))
            
            # Create new column names
            xlRepair =
                xlDf %>%
                {tibble(colname = unlist(.[1, ]), date = unlist(.[2, ]))} %>%
                tidyr::fill(., colname) %>%
                dplyr::mutate(
                    .,
                    varname = str_to_lower(colname),
                    # https://stackoverflow.com/questions/4389644/regex-to-match-string-containing-two-names-in-any-order
                    # Repair varnames
                    varname = ifelse(str_detect(date, 'Organization'), 'fcfullname', varname),
                    varname = ifelse(str_detect(varname, '(?=.*gdp)(?=.*quarterly)'), 'gdp', varname),
                    varname = ifelse(str_detect(varname, '(?=.*fed)(?=.*funds)'), 'ffr', varname),
                    varname = ifelse(str_detect(varname, 'cpi'), 'inf', varname),
                    varname = ifelse(str_detect(varname, 'unemployment'), 'ue', varname),
                    # Keep varname
                    keep = varname %in% c('fcfullname', 'gdp', 'ffr', 'inf', 'ue')
                ) %>%
                dplyr::mutate(
                    .,
                    # Repair dates
                    date = str_replace_all(
                        date,
                        c(
                            'Fourth Quarter ' = '10 ', 'Third Quarter ' = '7 ',
                            'Second Quarter ' = '4 ', 'First Quarter ' = '1 ',
                            setNames(paste0(1:12, ' '), paste0(month.abb, ' ')),
                            setNames(paste0(1:12, ' '), paste0(month.name, ' '))
                            )
                        ),
                    date =
                        ifelse(
                            !date %in% c('Name:', 'Organization:') & keep == TRUE,
                            paste0(
                                str_sub(date, -4),
                                '-',
                                str_pad(str_squish(str_sub(date, 1, nchar(date) - 4)), 2, pad = '0'),
                                '-01'
                                ),
                            NA
                        ),
                    date = as_date(date)
                    )

            
            df =
                suppressMessages(readxl::read_excel(
                    dest,
                    col_names = paste0(xlRepair$varname, '_', xlRepair$date),
                    na = c('', ' ', '-', 'NA'),
                    skip = 2
                    )) %>%
                # Only keep columns selected as keep = TRUE in last step
                dplyr::select(
                    .,
                    xlRepair %>% dplyr::mutate(., index = 1:nrow(.)) %>% dplyr::filter(., keep == TRUE) %>% .$index
                    ) %>%
                dplyr::rename(., 'fcfullname' = 1) %>%
                # Bind WSJ row - select last vintage
                dplyr::mutate(., fcfullname = ifelse(fcfullname %in% month.name, 'WSJ Consensus', fcfullname)) %>%
                dplyr::filter(., fcfullname %in% orgsDf$fcfullname) %>%
                {
                    dplyr::bind_rows(
                        dplyr::filter(., !fcfullname %in% 'WSJ Consensus'),
                        dplyr::filter(., fcfullname %in% 'WSJ Consensus') %>% head(., 1)
                    )
                } %>%
                dplyr::mutate(., across(-fcfullname, as.numeric)) %>%
                tidyr::pivot_longer(., -fcfullname, names_sep = '_', names_to = c('varname', 'date')) %>%
                dplyr::mutate(., date = as_date(date)) %>%
                # Now split and fill in frequencies and quarterly data
                dplyr::group_by(., fcfullname, varname) %>%
                dplyr::group_split(.) %>%
            	purrr::keep(., function(z) nrow(dplyr::filter(z, !is.na(value))) > 0 ) %>% # Cull completely empty data frames
                purrr::map_dfr(., function(z)
                    tibble(
                        date = seq(from = min(na.omit(z)$date), to = max(na.omit(z)$date), by = '3 months')
                        ) %>%
                        dplyr::left_join(., z, by = 'date') %>%
                        dplyr::mutate(., value = zoo::na.approx(value)) %>%
                        dplyr::mutate(
                            .,
                            fcfullname = unique(z$fcfullname),
                            varname = unique(z$varname),
                            freq = 'q',
                            form = 'd1',
                            vdate = as_date(x$date)
                            )
                    ) %>%
                dplyr::right_join(., orgsDf, by = 'fcfullname') %>%
                dplyr::select(., -fcfullname)
        }) %>%
        dplyr::bind_rows(.)

            
    m$ext$sources$wsj <<- df   
})
```


## CBO Forecasts
```{r}
local({

  urlDf =
      httr::GET('https://www.cbo.gov/data/budget-economic-data') %>%
      httr::content(., type = 'parsed') %>%
      xml2::read_html(.) %>%
      rvest::html_nodes('div .view-content') %>%
      .[[9]] %>%
      rvest::html_nodes(., 'a') %>%
      purrr::map_dfr(., function(x) tibble(date = rvest::html_text(x), url = rvest::html_attr(x, 'href'))) %>%
      dplyr::transmute(
          .,
          date =
              paste0(
                  str_sub(date, -4), '-',
                  str_pad(match(str_sub(date, 1, 3), month.abb), 2, pad = '0'),
                  '-01'
                  ),
          url
      ) %>%
      dplyr::mutate(., date = as.Date(date)) %>%
      dplyr::filter(., date >= as.Date('2018-01-01'))

    
  tempPath = file.path(DL_DIR, 'cbo.xlsx')
  
  paramsDf =
    tribble(
      ~ varname, ~ cboCategory, ~ cboname, ~ cboUnits,
      'gdp', 'Output', 'Real GDP', 'Percentage change, annual rate',
      'pcepi', 'Prices', 'Price Index, Personal Consumption Expenditures (PCE)', 'Percentage change, annual rate',
      'inf', 'Prices', 'Consumer Price Index, All Urban Consumers (CPI-U)', 'Percentage change, annual rate',
      'wti', 'Prices', 'Price of Crude Oil, West Texas Intermediate (WTI)', 'Dollars per barrel',
      'ue', 'Labor', 'Unemployment Rate, Civilian, 16 Years or Older', 'Percent',
      'ffr', 'Interest Rates', 'Federal Funds Rate', 'Percent',
      'pce', 'Components of GDP (Real)', 'Personal Consumption Expenditures', 'Percentage change, annual rate',
      't10y', 'Interest Rates', '10-Year Treasury Note', 'Percent',
      't03m', 'Interest Rates', '3-Month Treasury Bill', 'Percent'
      )
  
  
  df =
    urlDf %>%
    purrr::transpose(.) %>%
    lapply(., function(x) {

      download.file(x$url, tempPath, mode = 'wb', quiet = TRUE)
          
      # Starts earlier form Jan 2019
      xl =
        suppressMessages(readxl::read_excel(
          tempPath,
          sheet = '1. Quarterly',
          skip = {if (as.Date(x$date, origin = lubridate::origin) == '2019-01-01') 5 else 6}
          )) %>%
        dplyr::rename(., cboCategory = 1, cboname = 2, cboname2 = 3, cboUnits = 4) %>%
        dplyr::mutate(., cboname = ifelse(is.na(cboname), cboname2, cboname)) %>%
        dplyr::select(., -cboname2) %>%
        tidyr::fill(., cboCategory, .direction = 'down') %>%
        tidyr::fill(., cboname, .direction = 'down') %>%
        na.omit(.)

      
      xl %>%
        dplyr::inner_join(., paramsDf, by = c('cboCategory', 'cboname', 'cboUnits')) %>%
        dplyr::select(., -cboCategory, -cboname, -cboUnits) %>%
        tidyr::pivot_longer(-varname, names_to = 'obsDate') %>%
        dplyr::mutate(., obsDate = econforecasting::strdateToDate(obsDate)) %>%
        dplyr::filter(., obsDate >= as.Date(x$date, origin = lubridate::origin)) %>%
        dplyr::mutate(., vintageDate = as.Date(x$date, origin = lubridate::origin))
      }) %>%
    dplyr::bind_rows(.) %>%
    dplyr::transmute(., fcname = 'cbo', varname, form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)
  
  # Count number of forecasts per group
  # df %>% dplyr::group_by(vintageDate, varname) %>% dplyr::summarize(., n = n()) %>% View(.)
  
    
  m$ext$sources$cbo <<- df   
})
```


## EINF Model - Cleveland Fed
```{r}
local({
	
  file = file.path(DL_DIR, paste0('inf.xls'))

  download.file('https://www.clevelandfed.org/en/our-research/indicators-and-data/~/media/content/our%20research/indicators%20and%20data/inflation%20expectations/ie%20latest/ie%20xls.xls', file, mode = 'wb')

  df =
	readxl::read_excel(file, sheet = 'Expected Inflation') %>%
  	dplyr::rename(., vintageDate = 'Model Output Date') %>%
  	tidyr::pivot_longer(., -vintageDate, names_to = 'ttm', values_to = 'yield') %>%
  	dplyr::mutate(
  		.,
  		vintageDate = as.Date(vintageDate), ttm = as.numeric(str_replace(str_sub(ttm, 1, 2), ' ', '')) * 12
  		) %>%
  	dplyr::filter(., vintageDate == max(vintageDate)) %>%
  	dplyr::right_join(., tibble(ttm = 1:360), by = 'ttm') %>%
  	dplyr::arrange(., ttm) %>%
  	dplyr::mutate(
  		.,
  		yield = zoo::na.spline(yield),
  		vintageDate = unique(na.omit(vintageDate)),
  		curDate = floor_date(p$VINTAGE_DATE, 'months'),
  		cumReturn = (1 + yield)^(ttm/12),
  		yttmAheadCumReturn = dplyr::lead(cumReturn, 1)/cumReturn,
  		yttmAheadAnnualizedYield = (yttmAheadCumReturn^(12/1) - 1) * 100,
  		obsDate = add_with_rollback(curDate, months(ttm - 1))
  		) %>%
  	dplyr::transmute(
  		.,
		fcname = 'cle',
		varname = 'inf',
        form = 'd1',
		freq = 'm',
		date = obsDate,
  		vdate = vintageDate,
  		value = yttmAheadAnnualizedYield,
  		) %>%
  	na.omit(.)

    m$ext$sources$cle <<- df
})
```

## CME Model
```{r}
local({
  
	# First get from Quandl
	message('Starting Quandl data scrape...')
	df =
		lapply(1:24, function(j) {
			# message(j)
			read_csv(
				paste0(
					'https://www.quandl.com/api/v3/datasets/CHRIS/CME_FF', j,
					'.csv?api_key=', CONST$QUANDL_API_KEY
					),
				col_types = 'Ddddddddd'
				) %>%
				dplyr::transmute(., vintageDate = Date, settle = Settle, j = j) %>%
				dplyr::filter(., vintageDate >= as.Date('2010-01-01'))
			}) %>%
		dplyr::bind_rows(.) %>%
		dplyr::transmute(
			.,
			vintageDate,
		# Consider the forecasted period the vintageDate + j
			obsDate =
				econforecasting::strdateToDate(paste0(year(vintageDate), 'M', month(vintageDate))) %>%
				lubridate::add_with_rollback(., months(j - 1), roll_to_first = TRUE),
			value = 100 - settle,
			varname = 'ffr',
			fcname = 'cme'
			)
	
	message('Completed Quandl data scrape')
	
	message('Starting CME data scrape...')
	cookieVal =
	    httr::GET(
            'https://www.cmegroup.com/',
            add_headers(c(
                'User-Agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
                'Accept'= 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding' = 'gzip, deflate, br',
                'Accept-Language' ='en-US,en;q=0.5',
                'Cache-Control'='no-cache',
                'Connection'='keep-alive',
                'DNT' = '1'
                ))
            ) %>%
	    httr::cookies(.) %>% as_tibble(.) %>% dplyr::filter(., name == 'ak_bmsc') %>% .$value
	
	
	# Get CME Vintage Date
	lastTradeDate =
		httr::GET(
			paste0('https://www.cmegroup.com/CmeWS/mvc/Quotes/Future/305/G?quoteCodes=null&_='),
			add_headers(c(
				'User-Agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
				'Accept'= 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
				'Accept-Encoding' = 'gzip, deflate, br',
				'Accept-Language' ='en-US,en;q=0.5',
				'Cache-Control'='no-cache',
				'Connection'='keep-alive',
				'Cookie'=cookieVal,
				'DNT' = '1',
				'Host' = 'www.cmegroup.com'
			))
		) %>% content(., 'parsed') %>% .$tradeDate %>% lubridate::parse_date_time(., 'd-b-Y') %>% as_date(.)
		
	
	df2 =
		tribble(
			~ varname, ~ cmeId,
			'ffr', '305',
			'sofr', '8462',
			'sofr', '8463'
		) %>%
		purrr::transpose(.) %>%
		purrr::map_dfr(., function(var) {
			# message(var)
			content =
				httr::GET(
					paste0('https://www.cmegroup.com/CmeWS/mvc/Quotes/Future/', var$cmeId, '/G?quoteCodes=null&_='),
					# 3/30/21 - CME website now requires user-agent header
					add_headers(c(
						'User-Agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
						'Accept'= 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
						'Accept-Encoding' = 'gzip, deflate, br',
						'Accept-Language' ='en-US,en;q=0.5',
						'Cache-Control'='no-cache',
						'Connection'='keep-alive',
						'Cookie'=cookieVal,
						'DNT' = '1',
						'Host' = 'www.cmegroup.com'
						))
					) %>%
					httr::content(., as = 'parsed')
			
			content %>%
				.$quotes %>%
				purrr::map_dfr(., function(x) {
					if (x$priorSettle %in% c('0.00', '-')) return() # Whack bug in CME website
					tibble(
						obsDate = lubridate::ymd(x$expirationDate),
						value = 100 - as.numeric(x$priorSettle),
						varname = var$varname
						)
					}) %>%
				return(.)
			}) %>% 
		# Now average out so that there's only one value for each (varname, obsDate) combo
		dplyr::group_by(varname, obsDate) %>%
		dplyr::summarize(., value = mean(value), .groups = 'drop') %>%
		dplyr::arrange(., obsDate) %>%
		# Get rid of forecasts for old observations
		dplyr::filter(., obsDate >= lubridate::floor_date(Sys.Date(), 'month')) %>%
		# Assume vintagedate is the same date as the last Quandl obs
		dplyr::mutate(., vintageDate = lastTradeDate, fcname = 'cme') %>%
		dplyr::filter(., value != 100)
	
	
	message('Completed CME data scrape...')
	
	# Now combine, replacing df2 with df1 if necessary
	combinedDf =
		dplyr::full_join(df, df2, by = c('fcname', 'vintageDate', 'obsDate', 'varname')) %>%
		# Use quandl data if available, otherwise use other data
		dplyr::mutate(., value = ifelse(!is.na(value.x), value.x, value.y)) %>%
		dplyr::select(., -value.x, -value.y)

	# Most data starts in 88-89, except j=12 which starts at 1994-01-04. Misc missing obs until 2006.
# 	df %>%
#   	tidyr::pivot_wider(., names_from = j, values_from = settle) %>%
#     	dplyr::arrange(., date) %>% na.omit(.) %>% dplyr::group_by(year(date)) %>% dplyr::summarize(., n = n()) %>%
# 		View(.)
	
	
	## Add monthly interpolation
	message('Adding monthly interpolation ...')

	finalDf =
		combinedDf %>%
		dplyr::group_by(vintageDate, varname) %>%
		dplyr::group_split(.) %>%
		lapply(., function(x) {
			x %>%
				# Join on missing obs dates
				dplyr::right_join(
					.,
					tibble(obsDate = seq(from = .$obsDate[[1]], to = tail(.$obsDate, 1), by = '1 month')) %>%
						dplyr::mutate(n = 1:nrow(.)),
					by = 'obsDate'
					) %>%
				dplyr::arrange(obsDate) %>%
				dplyr::transmute(
					.,
					fcname = head(fcname, 1),
					varname = head(varname, 1),
					form = 'd1',
					freq = 'm',
					date = obsDate,
					vdate = head(vintageDate, 1),
					value = zoo::na.spline(value)
					)		
			}) %>%
		dplyr::bind_rows(.)

	m$ext$sources$cme <<- finalDf
})
```

## DNS - TDNS1, TDNS2, TDNS3, Treasury Yields, and Spreads
DIEBOLD LI FUNCTION SHOULD BE ffr + f1 + f2 () + f3()
Calculated TDNS1: TYield_10y 
Calculated TDNS2: -1 * (t10y - t03m)
Calculated TDNS3: .3 * (2*t02y - t03m - t10y)
Keep these treasury yield forecasts as the external forecasts ->
note that later these will be "regenerated" in the baseline calculation, may be off a bit due to calculation from TDNS, compare to 
```{r}
local({
    
    dnsCoefs = m$dnsCoefs
    dnsLambda = m$dnsLambda
    dnsYieldCurveNamesMap = m$dnsYieldCurveNamesMap
    
    # Monthly forecast up to 10 years
    # Get cumulative return starting from curDate
    fittedCurve =
        tibble(ttm = seq(1: 480)) %>%
        dplyr::mutate(., curDate = floor_date(p$VINTAGE_DATE, 'months')) %>%
        dplyr::mutate(
          .,
          annualizedYield = 
            dnsCoefs$b1 +
            dnsCoefs$b2 * (1-exp(-1 * dnsLambda * ttm))/(dnsLambda * ttm) +
            dnsCoefs$b3 * ((1-exp(-1 * dnsLambda * ttm))/(dnsLambda * ttm) - exp(-1 * dnsLambda * ttm)),
          # Get cumulative yield
          cumReturn = (1 + annualizedYield/100)^(ttm/12)
          )

    # Test for 20 year forecast
    # fittedCurve %>% dplyr::mutate(., futNetYield = dplyr::lead(annualizedYield, 240)/cumReturn, futYield = (futNetYield^(12/240) - 1) * 100) %>% dplyr::filter(., ttm < 120) %>% ggplot(.) + geom_line(aes(x = ttm, y = futYield))
    
    # fittedCurve %>% dplyr::mutate(., futYield = (dplyr::lead(cumYield, 3)/cumYield - 1) * 100)
    
    # Iterate over "yttms" tyield_1m, tyield_3m, ..., etc.
    # and for each, iterate over the original "ttms" 1, 2, 3, 
    # ..., 120 and for each forecast the cumulative return for the yttm period ahead.
    df0 =
        dnsYieldCurveNamesMap$ttm %>%
        lapply(., function(yttm)
            fittedCurve %>%
                dplyr::mutate(
                    .,
                    yttmAheadCumReturn = dplyr::lead(cumReturn, yttm)/cumReturn,
                    yttmAheadAnnualizedYield = (yttmAheadCumReturn^(12/yttm) - 1) * 100
                    ) %>%
                dplyr::filter(., ttm <= 120) %>%
                dplyr::mutate(., yttm = yttm) %>%
                dplyr::inner_join(., dnsYieldCurveNamesMap, c('yttm' = 'ttm'))
            ) %>%
        dplyr::bind_rows(.) %>%
        dplyr::mutate(
            .,
            date = add_with_rollback(curDate, months(ttm - 1))
            ) %>%
        dplyr::transmute(
            .,
            fcname = 'dns',
            varname,
            date,
            form = 'd1',
            freq = 'm',
            vdate = p$VINTAGE_DATE,
            value = yttmAheadAnnualizedYield
            )
    
    # Add ffr to forecasts
    df1 =
        df0 %>%
        dplyr::select(., varname, date, value) %>%
        dplyr::left_join(
            .,
            m$ext$sources$cme %>%
                dplyr::filter(., vdate == max(vdate)) %>%
                dplyr::filter(., varname == 'ffr') %>%
                dplyr::transmute(., ffr = value, date),
            by = 'date'
        ) %>%
        dplyr::mutate(., value = value + ffr) %>%
        dplyr::transmute(
            .,
            fcname = 'dns',
            varname,
            date,
            form = 'd1',
            freq = 'm',
            vdate = p$VINTAGE_DATE,
            value
            )

    # Calculate TDNS yield forecasts
    # Forecast vintage date should be bound to historical data vintage date since reliant purely on historical data
    df2 =
        df0 %>%
        dplyr::select(., varname, date, value) %>%
        tidyr::pivot_wider(., names_from = 'varname') %>%
        dplyr::transmute(
            .,
            date,
            tdns1 = t10y,
            tdns2 = -1 * (t10y - t03m),
            tdns3 = .3 * (2 * t02y - t03m - t10y)
            ) %>%
        tidyr::pivot_longer(., -date, names_to = 'varname') %>%
        dplyr::transmute(., fcname = 'dns', varname, date, form = 'd1', freq = 'm', vdate = p$VINTAGE_DATE, value)
    
    m$ext$sources$dns <<- dplyr::bind_rows(df1, df2)
})
```

# Finalize

## Export
```{r}
local({
  
    saveRDS(
    	list(p = p, h = h, m = m),
    	str_glue(OUTPUT_DIR, '/[{p$VINTAGE_DATE}] m1.rds')
    	)
    
})
```

