# Set Constants
```{r}
DIR = 'D:/Onedrive/__Projects/econforecasting'
DL_DIR = 'D:/Onedrive/__Projects/econforecasting/tmp'
PACKAGE_DIR = 'D:/Onedrive/__Projects/econforecasting/r-package' # Path to package with helper functions
INPUT_DIR = 'D:/Onedrive/__Projects/econforecasting/model-inputs' # Path to directory with constants.r (SQL DB info, SFTP info, etc.)
RESET_ALL = FALSE
```


# Initialize
```{r}
library(tidyverse)
library(data.table)
library(devtools)
library(jsonlite)
library(lubridate)
library(httr)
library(rvest)
library(DBI)
devtools::load_all(path = PACKAGE_DIR)
devtools::document(PACKAGE_DIR)

setwd(DIR)

if (dir.exists(DL_DIR)) unlink(DL_DIR, recursive = TRUE)
dir.create(DL_DIR, recursive = TRUE)

source(file.path(INPUT_DIR, 'constants.r'))

ef = list(
	variablesDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'all-variables'),
	forecastsDf = readxl::read_excel(file.path(INPUT_DIR, 'inputs.xlsx'), sheet = 'external-forecasts'),
	h = list(),
	e = list(),
	f = list()
	) %>%
	c(., list(variables = purrr::transpose(.$variablesDf, .names = .$variablesDf$varname)))
```



# Get Historical Data

## FRED
```{r}
local({
	
	fredRes =
		ef$variables %>%
		purrr::keep(., ~ .$source == 'fred') %>%
		lapply(., function(x) {
			
			message('Getting data ... ', x$varname)

			# Get series data
			dataDf =
				econforecasting::getDataFred(x$sckey, CONST$FRED_API_KEY, .freq = x$freq) %>%
				dplyr::transmute(., date = obsDate, value) %>%
				dplyr::filter(., date >= as.Date('2010-01-01'))

			# Get series release
			releaseDf =
				httr::RETRY(
					'GET', 
					paste0(
						'https://api.stlouisfed.org/fred/series/release?',
						'series_id=',x$sckey,'&api_key=',CONST$FRED_API_KEY,'&file_type=json'
						),
					times = 10
				) %>%
		        httr::content(., as = 'parsed') %>%
				.$releases %>%
				.[[1]] %>%
				as_tibble(.)

			list(dataDf = dataDf, releaseDf = releaseDf)
			})
	
	
	for (varname in names(fredRes)) {
		ef$variables[[varname]]$rawData <<- fredRes[[varname]]$dataDf
		ef$variables[[varname]]$releasekey <<- fredRes[[varname]]$releaseDf$id
	}
})
```

## Yahoo Finance
```{r}
local({
	
	res =
		ef$variables %>%
    	purrr::keep(., ~ .$source == 'yahoo') %>%
		lapply(., function(x) {
			url =
				paste0(
					'https://query1.finance.yahoo.com/v7/finance/download/', x$sckey,
					'?period1=', '946598400', # 12/30/1999
					'&period2=', as.numeric(as.POSIXct(Sys.Date() + lubridate::days(1))),
					'&interval=1d',
					'&events=history&includeAdjustedClose=true'
				)
			data.table::fread(url, showProgress = FALSE) %>%
				.[, c('Date', 'Adj Close')]	%>%
				setnames(., new = c('date', 'value')) %>%
				as_tibble(.) %>%
				return(.)
		})
	
	for (varname in names(res)) {
		ef$variables[[varname]]$rawData <<- res[[varname]]
	}
})
```



# Aggregate Frequencies

## Move to Hist Object
```{r}
local({
	
	 res =
	 	ef$variables %>%
	 	purrr::keep(., ~ is_tibble(.$rawData)) %>%
	 	lapply(., function(x)
	 		list(x$rawData) %>%
	 			setNames(., x$freq)
	 		)
	 
	for (varname in names(res)) {
		ef$variables[[varname]]$h$base <<- res[[varname]]
	}
})
```

## Monthly Agg
```{r}
local({
	
    res =
        # Get all daily/weekly varnames with pre-existing data
        ef$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w') && is_tibble(.$rawData)) %>%
        # Add monthly values for current month
        lapply(., function(x) {
        	x$rawData %>%
        		dplyr::mutate(., date = as.Date(paste0(year(date), '-', month(date), '-01'))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop') %>%
        		dplyr::mutate(., freq = 'm') %>%
        		{
        			# Keep last-month aggregation despite possible mising data if set in inputs.xlsx
        			if (x$append_eom_with_currentval == 1) .
        			else dplyr::filter(., date != max(date))
        		}
        	})
    
	for (varname in names(res)) {
		ef$variables[[varname]]$h$base$m <<- res[[varname]]
	}
})
```

## Quarterly Aggregation
```{r}
local({
	
    res =
        ef$variables %>%
        purrr::keep(., ~ .$freq %in% c('d', 'w', 'm') && is_tibble(.$rawData)) %>%
        lapply(., function(x) {
        	message(x$varname)
        	x$h$base$m %>%
        		dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
        		dplyr::group_by(., date) %>%
        		dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
        		# Only keep if all 3 monthly data exists (or imputed by previous chunk)
        		dplyr::filter(., n == 3) %>%
        		dplyr::select(., -n)
        	})
    
	for (varname in names(res)) {
		ef$variables[[varname]]$h$base$q <<- res[[varname]]
	}
    
})
```



# Add Calculated Variables

## Trailing Inflation
```{r}
local({
	
	mDf =
		ef$variables$cpi$h$base$m %>%
    	dplyr::mutate(., value = (value/dplyr::lag(value, 13) - 1) * 100) %>%
		na.omit(.)
	
	qDf =
		mDf %>%
		dplyr::mutate(., date = strdateToDate(paste0(year(date), 'Q', quarter(date)))) %>%
	    dplyr::group_by(., date) %>%
	    dplyr::summarize(., value = mean(value), .groups = 'drop', n = n()) %>%
	    # Only keep if all 3 monthly data exists (or imputed by previous chunk)
	    dplyr::filter(., n == 3) %>%
	    dplyr::select(., -n)

	ef$variables$inf$h$base$m <<- mDf
	ef$variables$inf$h$base$q <<- qDf
})
```


## DNS Model - Interest Rates
Let tyield = ffr + dns_curve(ttm)
Exogenously choose ffr and tyield_10y_3m (negative of 10year - 3month spread; 3-month driven heavily by ffr)
```{r}
local({
  
  # Create tibble mapping tyield_3m to 3, tyield_1y to 12, etc.
  yieldCurveNamesMap =
    ef$variables %>% 
    map_chr(., ~.$varname) %>%
    unique(.) %>%
    purrr::keep(., ~ str_sub(., 1, 1) == 't' & str_length(.) == 4) %>%
    tibble(varname = .) %>%
    dplyr::mutate(., ttm = as.numeric(str_sub(varname, 2, 3)) * ifelse(str_sub(varname, 4, 4) == 'y', 12, 1))
  
  # Create training dataset from SPREAD from ffr - fitted on last 3 months
  trainDf =
	purrr::map_dfr(yieldCurveNamesMap$varname, function(x) ef$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)) %>%
  	dplyr::select(., -freq) %>%
  	dplyr::filter(., date >= add_with_rollback(Sys.Date(), months(-3))) %>%
    dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
    dplyr::left_join(., dplyr::transmute(ef$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
  	dplyr::mutate(., value = value - ffr) %>%
  	dplyr::select(., -ffr)
  
  # @param df: (tibble) A tibble continuing columns obsDate, value, and ttm
  # @param returnAll: (boolean) FALSE by default.
  # If FALSE, will return only the MAPE (useful for optimization).
  # Otherwise, will return a tibble containing fitted values, residuals, and the beta coefficients.
  getDnsFit = function(df, lambda, returnAll = FALSE) {
    df %>%
      dplyr::mutate(
        .,
        f1 = 1,
        f2 = (1 - exp(-1 * lambda * ttm))/(lambda * ttm),
        f3 = f2 - exp(-1 * lambda * ttm)
        ) %>%
      dplyr::group_by(date) %>%
      dplyr::group_split(.) %>%
      lapply(., function(x) {
        reg = lm(value ~ f1 + f2 + f3 - 1, data = x)
        dplyr::bind_cols(x, fitted = fitted(reg)) %>%
          dplyr::mutate(., b1 = coef(reg)[['f1']], b2 = coef(reg)[['f2']], b3 = coef(reg)[['f3']]) %>%
          dplyr::mutate(., resid = value - fitted)
        }) %>%
      dplyr::bind_rows(.) %>%
      {
        if (returnAll == FALSE) dplyr::summarise(., mae = mean(abs(resid))) %>% .$mae
        else .
        } %>%
      return(.)
  }

  # Find MAPE-minimizing lambda value
  optimLambda =
    optimize(
      getDnsFit,
      df = trainDf,
      returnAll = FALSE,
      interval = c(-1, 1),
      maximum = FALSE
      )$minimum
  
  mDf =
	purrr::map_dfr(yieldCurveNamesMap$varname, function(x) ef$variables[[x]]$h$base$m %>% dplyr::mutate(., varname = x)) %>%
    dplyr::select(., -freq) %>%
    dplyr::right_join(., yieldCurveNamesMap, by = 'varname') %>%
    dplyr::left_join(., dplyr::transmute(ef$variables$ffr$h$base$m, date, ffr = value), by = 'date') %>%
    dplyr::mutate(., value = value - ffr) %>%
    dplyr::select(., -ffr) %>%
  	getDnsFit(., lambda = optimLambda, returnAll = TRUE) %>%
  	dplyr::group_by(., date) %>%
  	dplyr::summarize(., tdns1 = unique(b1), tdns2 = unique(b2), tdns3 = unique(b3)) %>%
  	tidyr::pivot_longer(., -date, names_to = 'varname')

  qDf = mDf %>% monthlyDfToQuarterlyDf(.)
  
  mDfs = mDf %>% split(., as.factor(.$varname)) %>% lapply(., function(x) x %>% dplyr::select(., -varname))
  
  qDfs = qDf %>% split(., as.factor(.$varname)) %>% lapply(., function(x) x %>% dplyr::select(., -varname))
  	

  for (varname in names(mDfs)) {
	ef$variables[[varname]]$h$base$m <<- mDfs[[varname]]
	ef$variables[[varname]]$h$base$q <<- qDfs[[varname]]
  }
})
```

## Other Interest Rate Spreads
```{r}
local({
    
    resDfs =
        tribble(
            ~varname, ~var1, ~var2,
            't03mffrspread', 't03m', 'ffr',
            't10yt03mspread', 't10y', 't03m',
            'mort30ymort15yspread', 'mort30y', 'mort15y',
            'mort15yt10yspread', 'mort15y', 't10y'
            ) %>%
        purrr::transpose(., .names = .$varname) %>%
        lapply(., function(x) {
            m =
                dplyr::inner_join(
                    ef$variables[[x$var1]]$h$base$m,
                    ef$variables[[x$var2]]$h$base$m %>% dplyr::rename(., v2 = value),
                    by = 'date'
                ) %>%
                dplyr::transmute(., date, value = value - v2)
            q = monthlyDfToQuarterlyDf(m)
            list(m = m, q = q)
            })
    
  for (varname in names(resDfs)) {
	ef$variables[[varname]]$h$base <<- resDfs[[varname]]
  }
})
```



# Transformations

## Deseasonalize
```{r}
local({
	# seasDf =
	# 	ef$h$sourceDf %>%
	# 	dplyr::filter(., varname == 'hpils') %>%
	# 	dplyr::mutate(
	# 		.,
	# 		seas = 
	# 		{ts(.$value, start = c(year(.$date[1]), month(.$date[1])), freq = 12)} %>%
	# 		seasonal::seas(.) %>%
	# 		predict(.)
	# 		) %>%
	# 	dplyr::select(., -value)
	# 
	# df =
	# 	dplyr::left_join(ef$h$sourceDf, seasDf, by = c('date', 'varname', 'freq')) %>%
	# 	dplyr::mutate(., value = ifelse(is.na(seas), value, seas)) %>%
	# 	dplyr::select(., -seas)
	# 
	# ef$h$seasDf <<- df
})
```

## Stationarity
```{r}
local({

	resDfs =
		ef$variables %>%
		lapply(., function(x) {
			
			message(x$varname)
			
			lapply(c('st', 'd1', 'd2') %>% setNames(., .), function(form) {
			
				if (x[[form]] == 'none') return(NULL)
				# Now iterate through sub-frequencies
				lapply(x$h$base, function(df)
					df %>%
						dplyr::arrange(., date) %>%
						dplyr::mutate(
							.,
							value = {
								if (x[[form]] == 'base') value
								else if (x[[form]] == 'dlog') dlog(value)
								else if (x[[form]] == 'diff1') diff1(value)
								else if (x[[form]] == 'pchg') pchg(value)
								else if (x[[form]] == 'apchg') apchg(value)
								else stop ('Error')
								}
							) %>%
						na.omit(.)
					)
				}) %>%
				purrr::compact(.)
			})

	for (varname in names(resDfs)) {
		for (form in names(resDfs[[varname]])) {
			ef$variables[[varname]]$h[[form]] <<- resDfs[[varname]][[form]]
		}
	}
})
```



# Aggregate

## Flat
```{r}
local({
	
	flatDf =
		purrr::imap_dfr(ef$variables, function(x, varname)
			purrr::imap_dfr(x$h, function(y, form)
				purrr::imap_dfr(y, function(z, freq)
					z %>% dplyr::mutate(., freq = freq, form = form, varname = varname)
					)
				)
			) %>%
		dplyr::filter(., freq %in% c('m', 'q'))

	ef$h$flatDf <<- flatDf
})
```


## Create monthly/quarterly matrixes
```{r}
local({
	
	wide =
		ef$h$flatDf %>%
		as.data.table(.) %>%
		split(., by = 'form') %>%
		lapply(., function(x)
			split(x, by = 'freq') %>%
				lapply(., function(y)
					as_tibble(y) %>%
						dplyr::select(., -freq, -form) %>%
						tidyr::pivot_wider(., names_from = varname) %>%
						dplyr::arrange(., date)
					)
			)

	ef$h$base <<- wide$base
	ef$h$st <<- wide$st
	ef$h$d1 <<- wide$d1
	ef$h$d2 <<- wide$d2
})
```


# Download External Forecasts
Download Existing Forecasts -> Convert to "O" Types

## Atlanta Fed
GDP, PCE
```{r}
local({
  
  ##### GDP #####
  paramsDf =
	tribble(
      ~ varname, ~ fredId, 
      'gdp', 'GDPNOW',
      'pce', 'PCECONTRIBNOW'
    )
  
  # GDPNow
  df =
  	lapply(paramsDf %>% purrr::transpose(.), function(x)
      getDataFred(x$fredId, CONST$FRED_API_KEY, .returnVintages = TRUE) %>%
        dplyr::filter(., obsDate >= .$vintageDate - months(3)) %>%
        dplyr::transmute(., fcname = 'atl', varname = x$varname, form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)
      ) %>%
    dplyr::bind_rows(.)

  ef$e$forecasts$atl <<- df
})
```

## St. Louis Fed
GDP
```{r}
local({
  
  df =
	  getDataFred('STLENI', CONST$FRED_API_KEY, .returnVintages = TRUE) %>%
	  dplyr::filter(., obsDate >= .$vintageDate - months(3)) %>%
    dplyr::transmute(., fcname = 'stl', varname = 'gdp', form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)

    
  ef$e$forecasts$stl <<- df
})
```

## New York Fed
GDP
```{r}
local({
  
  file = file.path(DL_DIR, 'nyf.xlsx')
  
  httr::GET(
    'https://www.newyorkfed.org/medialibrary/media/research/policy/nowcast/new-york-fed-staff-nowcast_data_2002-present.xlsx?la=en',
    httr::write_disk(file, overwrite = TRUE)
    )
  
  df =
    readxl::read_excel(file, sheet = 'Forecasts By Quarter', skip = 13) %>%
    dplyr::rename(., vintageDate = 1) %>%
    dplyr::mutate(., vintageDate = as.Date(vintageDate)) %>%
    tidyr::pivot_longer(., -vintageDate, names_to = 'obsDate', values_to = 'value') %>%
    na.omit(.) %>%
    dplyr::mutate(., obsDate = econforecasting::strdateToDate(obsDate)) %>%
    dplyr::transmute(., fcname = 'nyf', varname = 'gdp', form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)

  ef$e$forecasts$nyf <<- df
})
```

## Philadelphia Fed
```{r}
local({
  
  # Scrape vintage dates
  vintageDf =
    httr::GET('https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/spf-release-dates.txt?la=en&hash=B0031909EE9FFE77B26E57AC5FB39899') %>%
    httr::content(., as = 'text', encoding = 'UTF-8') %>%
    str_sub(
        .,
        str_locate(., coll('1990 Q2'))[1], str_locate(., coll('*The 1990Q2'))[1] - 1
        ) %>%
    readr::read_table(., col_names = FALSE) %>%
    tidyr::fill(., X1, .direction = 'down') %>%
    na.omit(.) %>%
    dplyr::transmute(
        .,
        releaseDate = econforecasting::strdateToDate(paste0(X1, X2)),
        vintageDate = lubridate::mdy(str_replace_all(str_extract(X3, "[^\\s]+"), '[*]', ''))
    ) %>%
    # Don't include first date - weirdly has same vintage date as second date
    dplyr::filter(., releaseDate >= as.Date('2000-01-01'))
  
  paramsDf =
    tribble(
      ~ varname, ~ spfname, ~ method,
      'gdp', 'RGDP', 'growth',
      'pce', 'RCONSUM', 'growth',
      'unemp', 'UNEMP', 'level',
      't03m', 'TBILL', 'level',
      't10y', 'TBOND', 'level',
      'hstarts', 'HOUSING', 'level'
    )
  
  
  df =
    lapply(c('level', 'growth'), function(m) {
      
      file = file.path(DL_DIR, paste0('spf-', m, '.xlsx'))

      httr::GET(
        paste0(
        'https://www.philadelphiafed.org/-/media/frbp/assets/surveys-and-data/survey-of-professional-forecasters/historical-data/median', m, '.xlsx?la=en'),
        httr::write_disk(file, overwrite = TRUE)
        )
      
      lapply(paramsDf %>% dplyr::filter(., method == m) %>% purrr::transpose(.), function(x) {
        readxl::read_excel(file, na = '#N/A', sheet = x$spfname) %>%
          dplyr::select(
            .,
            c('YEAR', 'QUARTER', {
              if (m == 'level') paste0(x$spfname, 2:6) else paste0('d', str_to_lower(x$spfname), 2:6)
              })
            ) %>%
          dplyr::mutate(., releaseDate = econforecasting::strdateToDate(paste0(YEAR, 'Q', QUARTER))) %>%
          dplyr::select(., -YEAR, -QUARTER) %>%
          tidyr::pivot_longer(., -releaseDate, names_to = 'fcPeriods') %>%
          dplyr::mutate(., fcPeriods = as.numeric(str_sub(fcPeriods, -1)) - 2) %>%
          dplyr::mutate(., obsDate = add_with_rollback(releaseDate, months(fcPeriods * 3))) %>%
          na.omit(.) %>%
          dplyr::inner_join(., vintageDf, by = 'releaseDate') %>%
          dplyr::transmute(., fcname = 'spf', varname = x$varname, form = 'd1', vdate = vintageDate, date = obsDate, value)
        }) %>%
        dplyr::bind_rows(.) %>%
        return(.)
      }) %>%
    dplyr::bind_rows(.)
    
    ef$e$forecasts$spf <<- df   
})
```

## WSJ Economic Survey
WSJ Survey Updated to Quarterly - see https://www.wsj.com/amp/articles/economic-forecasting-survey-archive-11617814998
```{r}
local({
    
    orgsDf =
        tibble(
          fcname = c('wsj', 'fnm', 'wfc', 'gsu'),
          fcnameFull = c('WSJ Consensus', 'Fannie Mae', 'Wells Fargo & Co.', 'Georgia State University')
        )
    
    filePaths =
        seq(as.Date('2020-01-01'), to = Sys.Date(), by = '3 months') %>%
        purrr::map(., function(x)
            list(
                date = x,
                file = paste0('wsjecon', str_sub(x, 6, 7), str_sub(x, 3, 4), '.xls')
                )
            )

    df =
        lapply(filePaths, function(x) {
            message(x$date)
            dest = file.path(DL_DIR, 'wsj.xls')

            download.file(
                paste0('https://online.wsj.com/public/resources/documents/', x$file),
                destfile = dest,
                mode = 'wb',
                quiet = TRUE
                )
            
            colStart =
              readxl::read_excel(dest, skip = 0, .name_repair = 'unique') %>%
              colnames(.) %>%
              {str_detect(., coll('GDP'))} %>% which(., arr.ind = TRUE) %>% .[1]
            
            colEnd =
              readxl::read_excel(dest, skip = 0, .name_repair = 'unique') %>%
              colnames(.) %>%
              {str_detect(., coll('...')) == FALSE} %>%
              which(., arr.ind = TRUE) %>% .[. > colStart] %>% min(.) %>% {. - 1}


            df =
                readxl::read_excel(dest, skip = 1, .name_repair = 'unique') %>%
                dplyr::rename(., fcnameFull = 2)


            for (i in 1:nrow(df)) {
                if (df[[i, 2]] %in% month.name) {
                    dataRow = i
                    break
                }
            }
            
            df %>%
                dplyr::filter(., fcnameFull %in% orgsDf$fcnameFull) %>%
                dplyr::bind_rows(df[dataRow, ] %>% dplyr::mutate(fcnameFull = 'WSJ Consensus')) %>%
                .[, c(2, colStart:colEnd)] %>%
                # Added asterisks to deal with 2020-05 dataset
                dplyr::mutate_at(., vars(-fcnameFull), function(y)
                  as.numeric(str_replace_all(y, coll('*'), ''))
                  ) %>%
                tidyr::pivot_longer(., -fcnameFull, names_to = 'obsDate') %>%
                # Get rid of ...16 coming from broken column names (see 2020-02 dataset)
                dplyr::mutate(
                    .,
                    obsDate =
                        ifelse(
                            str_detect(obsDate, coll('...')),
                            str_sub(obsDate, 1, str_locate(obsDate, coll('...')) - 1),
                            obsDate)
                    ) %>%
                dplyr::mutate(
                    .,
                    obsDate =
                        str_replace_all(
                            obsDate,
                            c('Fourth Quarter ' = '4', 'Third Quarter ' = '3',
                              'Second Quarter ' = '2', 'First Quarter ' = '1' )
                            )
                    ) %>%
                dplyr::mutate(
                    .,
                    obsDate =
                        paste0(
                            str_sub(obsDate, -4), '-',
                            str_pad(as.numeric(str_sub(obsDate, 1, 1)) * 3 - 2, 2, pad = '0'),
                            '-01')
                    ) %>%
                dplyr::transmute(., fcnameFull, date = as.Date(obsDate), vdate = as.Date(x$date), value)
            }) %>%
        dplyr::bind_rows(.) %>%
        dplyr::mutate(., varname = 'gdp', form = 'd1', freq = 'q') %>%
        dplyr::left_join(., orgsDf, by = 'fcnameFull') %>%
        dplyr::select(., -fcnameFull) %>%
      na.omit(.)
    
    # WSJ Concensus Forecasts
    # fromJSON('https://graphics.wsj.com/econforecast/data/data.php?f=fetchIndicator&i=0&r=12') %>%
    #     {tail(tibble(forecastDates = .$dates, values = .$est_values_avg[, 2]), -nrow(.$actuals))}

    ef$e$forecasts$wsj <<- df   
})
```

## CBO Forecasts
```{r}
local({

  urlDf =
      httr::GET('https://www.cbo.gov/data/budget-economic-data') %>%
      httr::content(., type = 'parsed') %>%
      xml2::read_html(.) %>%
      rvest::html_nodes('div .view-content') %>%
      .[[9]] %>%
      rvest::html_nodes(., 'a') %>%
      purrr::map_dfr(., function(x) tibble(date = rvest::html_text(x), url = rvest::html_attr(x, 'href'))) %>%
      dplyr::transmute(
          .,
          date =
              paste0(
                  str_sub(date, -4), '-',
                  str_pad(match(str_sub(date, 1, 3), month.abb), 2, pad = '0'),
                  '-01'
                  ),
          url
      ) %>%
      dplyr::mutate(., date = as.Date(date)) %>%
      dplyr::filter(., date >= as.Date('2018-01-01'))

    
  tempPath = file.path(DL_DIR, 'cbo.xlsx')
  
  paramsDf =
    tribble(
      ~ varname, ~ cboCategory, ~ cboname, ~ cboUnits,
      'gdp', 'Output', 'Real GDP', 'Percentage change, annual rate',
      'pcepi', 'Prices', 'Price Index, Personal Consumption Expenditures (PCE)', 'Percentage change, annual rate',
      'cpiu', 'Prices', 'Consumer Price Index, All Urban Consumers (CPI-U)', 'Percentage change, annual rate',
      'wti', 'Prices', 'Price of Crude Oil, West Texas Intermediate (WTI)', 'Dollars per barrel',
      'unemp', 'Labor', 'Unemployment Rate, Civilian, 16 Years or Older', 'Percent',
      'ffr', 'Interest Rates', 'Federal Funds Rate', 'Percent',
      'pce', 'Components of GDP (Real)', 'Personal Consumption Expenditures', 'Percentage change, annual rate'
      )
  
  
  df =
    urlDf %>%
    purrr::transpose(.) %>%
    lapply(., function(x) {

      download.file(x$url, tempPath, mode = 'wb', quiet = TRUE)
          
      # Starts earlier form Jan 2019
      xl =
        readxl::read_excel(
          tempPath,
          sheet = '1. Quarterly',
          skip = {if (as.Date(x$date, origin = lubridate::origin) == '2019-01-01') 5 else 6}
          ) %>%
        dplyr::rename(., cboCategory = 1, cboname = 2, cboname2 = 3, cboUnits = 4) %>%
        dplyr::mutate(., cboname = ifelse(is.na(cboname), cboname2, cboname)) %>%
        dplyr::select(., -cboname2) %>%
        tidyr::fill(., cboCategory, .direction = 'down') %>%
        tidyr::fill(., cboname, .direction = 'down') %>%
        na.omit(.)

      
      xl %>%
        dplyr::inner_join(., paramsDf, by = c('cboCategory', 'cboname', 'cboUnits')) %>%
        dplyr::select(., -cboCategory, -cboname, -cboUnits) %>%
        tidyr::pivot_longer(-varname, names_to = 'obsDate') %>%
        dplyr::mutate(., obsDate = econforecasting::strdateToDate(obsDate)) %>%
        dplyr::filter(., obsDate >= as.Date(x$date, origin = lubridate::origin)) %>%
        dplyr::mutate(., vintageDate = as.Date(x$date, origin = lubridate::origin))
      }) %>%
    dplyr::bind_rows(.) %>%
    dplyr::transmute(., fcname = 'cbo', varname, form = 'd1', freq = 'q', date = obsDate, vdate = vintageDate, value)
  
  # Count number of forecasts per group
  # df %>% dplyr::group_by(vintageDate, varname) %>% dplyr::summarize(., n = n()) %>% View(.)
  
  ef$e$forecasts$cbo <<- df
})
```


## EINF Model - Cleveland Fed
```{r}
local({
	
  file = file.path(DL_DIR, paste0('inf.xls'))

  download.file('https://www.clevelandfed.org/en/our-research/indicators-and-data/~/media/content/our%20research/indicators%20and%20data/inflation%20expectations/ie%20latest/ie%20xls.xls', file, mode = 'wb')

  df =
	readxl::read_excel(file, sheet = 'Expected Inflation') %>%
  	dplyr::rename(., vintageDate = 'Model Output Date') %>%
  	tidyr::pivot_longer(., -vintageDate, names_to = 'ttm', values_to = 'yield') %>%
  	dplyr::mutate(
  		.,
  		vintageDate = as.Date(vintageDate), ttm = as.numeric(str_replace(str_sub(ttm, 1, 2), ' ', '')) * 12
  		) %>%
  	dplyr::filter(., vintageDate == max(vintageDate)) %>%
  	dplyr::right_join(., tibble(ttm = 1:360), by = 'ttm') %>%
  	dplyr::arrange(., ttm) %>%
  	dplyr::mutate(
  		.,
  		yield = zoo::na.spline(yield),
  		vintageDate = unique(na.omit(vintageDate)),
  		curDate = floor_date(Sys.Date(), 'months'),
  		cumReturn = (1 + yield)^(ttm/12),
  		yttmAheadCumReturn = dplyr::lead(cumReturn, 1)/cumReturn,
  		yttmAheadAnnualizedYield = (yttmAheadCumReturn^(12/1) - 1) * 100,
  		obsDate = add_with_rollback(curDate, months(ttm - 1))
  		) %>%
  	dplyr::transmute(
  		.,
		fcname = 'cle',
		varname = 'inf',
        form = 'd1',
		freq = 'm',
		date = obsDate,
  		vdate = vintageDate,
  		value = yttmAheadAnnualizedYield,
  		) %>%
  	na.omit(.)

    ef$e$forecasts$cle <<- df
})
```


## CME Model
```{r}
local({
  
	# First get from Quandl
	message('Starting Quandl data scrape...')
	df =
		lapply(1:24, function(j) {
			# message(j)
			read_csv(
				paste0(
					'https://www.quandl.com/api/v3/datasets/CHRIS/CME_FF', j,
					'.csv?api_key=', CONST$QUANDL_API_KEY
					),
				col_types = 'Ddddddddd'
				) %>%
				dplyr::transmute(., vintageDate = Date, settle = Settle, j = j) %>%
				dplyr::filter(., vintageDate >= as.Date('2010-01-01'))
			}) %>%
		dplyr::bind_rows(.) %>%
		dplyr::transmute(
		.,
			vintageDate,
		# Consider the forecasted period the vintageDate + j
		obsDate =
			econforecasting::strdateToDate(paste0(year(vintageDate), 'M', month(vintageDate))) %>%
			lubridate::add_with_rollback(., months(j - 1), roll_to_first = TRUE),
		value = 100 - settle,
		varname = 'ffr',
		fcname = 'cme'
		)
	message('Completed Quandl data scrape')
	
	message('Starting CME data scrape...')
	cookieVal =
	    httr::GET(
            'https://www.cmegroup.com/',
            add_headers(c(
                'User-Agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
                'Accept'= 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Encoding' = 'gzip, deflate, br',
                'Accept-Language' ='en-US,en;q=0.5',
                'Cache-Control'='no-cache',
                'Connection'='keep-alive',
                'DNT' = '1'
                ))
            ) %>%
	    httr::cookies(.) %>% as_tibble(.) %>% dplyr::filter(., name == 'ak_bmsc') %>% .$value
	
	df2 =
		tribble(
			~ varname, ~ cmeId,
			'ffr', '305',
			'sofr', '8462',
			'sofr', '8463'
		) %>%
		purrr::transpose(.) %>%
		purrr::map_dfr(., function(var) {
			# message(var)
			httr::GET(
				paste0('https://www.cmegroup.com/CmeWS/mvc/Quotes/Future/', var$cmeId, '/G?quoteCodes=null&_='),
				# 3/30/21 - CME website now requires user-agent header
				add_headers(c(
					'User-Agent' = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:89.0) Gecko/20100101 Firefox/89.0',
					'Accept'= 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
					'Accept-Encoding' = 'gzip, deflate, br',
					'Accept-Language' ='en-US,en;q=0.5',
					'Cache-Control'='no-cache',
					'Connection'='keep-alive',
					'Cookie'=cookieVal,
					'DNT' = '1',
					'Host' = 'www.cmegroup.com'
					))
				) %>%
				httr::content(., as = 'parsed') %>%
				.$quotes %>%
				purrr::map_dfr(., function(x) {
					if (x$priorSettle %in% c('0.00', '-')) return() # Whack bug in CME website
					tibble(
						obsDate = lubridate::ymd(x$expirationDate),
						value = 100 - as.numeric(x$priorSettle),
						varname = var$varname
						)
					})
			}) %>% 
		# Now average out so that there's only one value for each (varname, obsDate) combo
		dplyr::group_by(varname, obsDate) %>%
		dplyr::summarize(., value = mean(value), .groups = 'drop') %>%
		dplyr::arrange(., obsDate) %>%
		# Get rid of forecasts for old observations
		dplyr::filter(., obsDate >= lubridate::floor_date(Sys.Date(), 'month')) %>%
		# Assume vintagedate is the same date as the last Quandl obs
		dplyr::mutate(., vintageDate = max(df$vintageDate), fcname = 'cme') %>%
		dplyr::filter(., value != 100) 
	message('Completed CME data scrape...')

	# Now combine, replacing df2 with df1 if necessary
	combinedDf =
		dplyr::full_join(df, df2, by = c('fcname', 'vintageDate', 'obsDate', 'varname')) %>%
		# Use quandl data if available, otherwise use other data
		dplyr::mutate(., value = ifelse(!is.na(value.x), value.x, value.y)) %>%
		dplyr::select(., -value.x, -value.y)

  # Most data starts in 88-89, except j=12 which starts at 1994-01-04. Misc missing obs until 2006.
  # df %>% tidyr::pivot_wider(., names_from = j, values_from = settle) %>% dplyr::arrange(., date) %>% na.omit(.) %>% dplyr::group_by(year(date)) %>% dplyr::summarize(., n = n()) %>% View(.)
	
	
	## Add monthly interpolation
	message('Adding monthly interpolation ...')

	finalDf =
		combinedDf %>%
		dplyr::group_by(vintageDate, varname) %>%
		dplyr::group_split(.) %>%
		lapply(., function(x) {
			x %>%
				# Join on missing obs dates
				dplyr::right_join(
					.,
					tibble(obsDate = seq(from = .$obsDate[[1]], to = tail(.$obsDate, 1), by = '1 month')) %>%
						dplyr::mutate(n = 1:nrow(.)),
					by = 'obsDate'
					) %>%
				dplyr::arrange(obsDate) %>%
				dplyr::transmute(
					.,
					fcname = head(fcname, 1),
					form = 'd1',
					freq = 'm',
					date = obsDate,
					value = zoo::na.spline(value),
					vdate = head(vintageDate, 1),
					varname = head(varname, 1),
					)		
			}) %>%
		dplyr::bind_rows(.) %>%
		dplyr::select(., -n)
  
	ef$e$forecasts$cme <<- finalDf
})
```


# Generate Initial Forecasts

## Create Initial Forecasts
```{r}
local({
    
    # Can use qualitative forecasts or just use raw inputs
    tffr3mspread = c()
    t10y3mspread = c()

})
```

