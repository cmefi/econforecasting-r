\documentclass[11pt, letterpaper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{xcolor}
\usepackage{geometry}
\usepackage[parfill]{parskip}
\usepackage{float}
\usepackage{graphicx}
\usepackage{fancyhdr}
\usepackage{appendix}
\graphicspath{{\Sexpr{paste0(DOC_DIR, '/images/')}}}
\newcommand{\vv}[1]{\textcolor{black}{\mathbf{#1}}}
\definecolor{econgreen}{RGB}{55, 91, 1}

\geometry{left=2.6cm, right = 2.6cm, top = 3cm, bottom = 3cm}

\fancypagestyle{plain}{
	\let\oldheadrule\headrule% Copy \headrule into \oldheadrule
	\renewcommand{\headrule}{\color{econgreen}\oldheadrule}
	\lhead{\small{\textcolor{black}{\leftmark}}}
	%\chead{}
	\rhead{\small{\textcolor{black}{\thepage}}}
	\lfoot{}
	\cfoot{}
	\rfoot{\textit{charles@cmefi.com}}
	\renewcommand{\headrulewidth}{0.5pt}
	%\renewcommand{\footrulewidth}{0.5pt}
}\pagestyle{plain}
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{titlepage}
\thispagestyle{empty}
\newgeometry{left=5cm, top=5cm} %defines the geometry for the titlepage
\pagecolor{econgreen}
\noindent
\includegraphics[width=2cm]{cmefi_short.png} \\[-1em]
\color{white}
\makebox[0pt][l]{\rule{1.3\textwidth}{1pt}}
\par
\noindent
%\textbf{\textsf{A Macroeconomic Nowcasting Model}} 
%\vskip5cm
{\Huge \textsf{A Nowcasting Model for Time Series with Ragged-Edge Data}}
\vskip\baselineskip
\noindent
\textsf{Model Run Date: \Sexpr{format(Sys.Date(), '%B %d, %Y')}}\\
\textsf{charles@cmefi.com}
\restoregeometry % restores the geometry
\nopagecolor% Use this to restore the color pages to white
\end{titlepage}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


<<echo=F,results='hide',message=F, warning=F>>=
    library(tidyverse)
    library(xtable)

    # Settings for PDF compilation
    knitr::opts_chunk$set(
      echo = F, results = 'asis',
      root.dir = OUTPUT_DIR,
      fig.path = paste0(OUTPUT_DIR, '/latex-figures/'),
      cache.path = paste0(OUTPUT_DIR, '/latex-cache/'),
      fig.width = 10, fig.height = 4, out.width = '5in', out.height = '2in',
      fig.pos = 'H', fig.align = 'center', warnings = F, message = F
      )
    options(xtable.include.rownames = F)
    options(xtable.size = 'footnotesize')
    options(xtable.table.placement = 'H')
    
    # rds = readRDS('D:/Onedrive/__Projects/econforecasting/nowcast.rds')

@

\tableofcontents
\newpage
\listoftables
\listoffigures
\newpage
\section{Motivation}
\textbf{Nowcasting} is the prediction of the present, the near future, and the near past. Nowcasting is important in economics because many important macroeconomic statistics are released with a lengthy delay. For example, the Bureau of Economic Analysis releases quarterly GDP data typically two months after the quarter has already ended - a significant delay for any companies or individuals who need the data for planning and forecast models.

This delay is particularly salient during times of high volatility. During the first few months of the COVID-19 pandemic in the United States, many companies attempted to use high-frequency indicators to attempt nowcasts of the state of the macroeconomy. For example, a major investment bank forecasted Q2 GDP growth of -3\% on March 3rd; the estimate was revised down to -14\% by March 21st, -25\% by March 25th, and -40\% by April 10th. Yet many such models were ad-hoc and only able to use a small number of predictive indicators, such as jobless claims or traffic data.

\textbf{Nowcasting is about deciphering key information about the state of the economy before official data is released.} Because of the fundamentally urgent nature of nowcasting, it is important that nowcast models exploit any latest, high-frequency data available. Nowcasts should be able to generate constantly rolling forecasts, updating these numbers in response to any new data releases.


For example, suppose the date is early March, and the variable we want to predict is Q1 GDP. The simplest way to predict Q1 GDP would be to use historical quarterly data from various economic variables. But this data would only go up to Q4 of last year, and would fail to capture the critically important predictive power that could be provided by monthly and daily data released throughout January and February. This lag in publication dates of different data series is known as the \textbf{ragged-edge} problem.

Suppose instead, we used monthly data as our predictors of Q1 GDP. Again, we will soon run in to a problem. As an example, imagine that we had imported 20 monthly data series. Suppose 5 of these series ended in December, 10 ended in January, and 5 ended in March. 

\begin{figure}[H]
\caption{Example of ragged-edge data}
\includegraphics[scale=.7]{nowcast-time-2}
\centering
\end{figure}

Traditional modeling methods would require us to either throw out variables or throw out months - for example, we could truncate all our data series at January and lose out the information provided by the 5 February data points. Alternatively, we could completely remove the 15 variables with data releasing before March. Both methods result in a serious loss of useful data and are unappealing.

In this paper, we will develop and utilize a methodology that will allow us to use the information from \textbf{all variables} at \textbf{any dates}. This model will give an updated forecast in response to any new data releases. Additionally, the model can be generalized to nowcast any time series variable, not just GDP. The methodology for the model will be described in the next section.

\newpage
\section{Methodology Overview}
Our goal will be to take monthly-frequency leading economic predictor variables --- industrial production, consumer sentiment, vehicle sales, and so on --- and use these to predict our quarterly variables. However, we will need a way to adjust for the ragged edges of the data.

\begin{figure}[H]
\caption{Overview of methodology (more detail in later sections)}
\includegraphics[scale=.7]{nowcast-time-3}
\centering
\end{figure}


To do this, we will use a methodology that relies heavily on principal components. It is well known that most macroeconomic variables are highly correlated with one another (see, for example, Bernanke et al 2005).
<<>>=
ef$h$q$st %>% dplyr::select(., vsales, csent, advsales, ipi, vix, spy, cfnai) %>% dplyr::rename(., 'VehicleSales' = vsales, 'ConsumerSentiment' = csent, 'AdvSales' = advsales, 'IndustrialProd' = ipi, 'VIX' = vix, 'SP500' = spy, 'CFNAI' = cfnai) %>% corrr::correlate(.) %>% corrr::shave(.) %>% dplyr::select(., -CFNAI) %>% xtable::xtable(., caption = 'Correlation Between Major Leading Predictors') %>% print(., size = 'scriptsize')
@
As a result, we can take a very high-dimensional dataset of indicator variables and extract a few time series, \textit{factors} or \textit{principal components}, using principal components analysis. These factors will be able to contain the majority of information within our larger, high-dimensional dataset. Then we will use a time series model of the factor behavior --- a vector autoregression, or VAR --- to forecast the factors forward in time. 

Finally, we will deal with the ragged edges of the data by casting our model dynamics in state-space form and using a Kalman filtration and smoothing process. This will, in essence, adjust our factor forecasts by whether or not the indicator variables used in the construction of the factors have been released. This procedure is a modified version of the two-step dynamic factor model utilized in Giannone et al (2008) and Doz et al (2011).

Finally, we will aggregate the smoothed and forecasted time series of factors up to a quarterly level. These now-quarterly forecasts of the factors can be used as covariates in an ARIMA model of the quarterly economic variables. The usage of factors as regressors of these economic variables is known as a \textit{dynamic factor model} (DFM). We additionally regularize our selection of regressors using a dynamic factor model.

The next section will go over the data and estimation procedure in detail. While this model can be generalized to nowcasts of other economic variables, we will use the example of nowcasting GDP and components of GDP.

\newpage
\section{Estimation Process}
The model is run every weekday; this documentation will use actual data and estimates from the most recent model run (\Sexpr{Sys.Date()}) to illustrate the procedure. The final nowcasted output is located in the results section.

\subsection{Data}
We begin by importing monthly data of various leading indicators from the St. Louis Federal Reserve Database (FRED). We choose data of at least monthly frequency and with historical data available as of at least 2010. Data are transformed for stationarity as listed below; \textit{dlog} refers to the natural log of the first difference, whereas \textit{base} means that no transformation was necessary for the variable to be stationary.

<<>>=
	ef$paramsDf %>%
    	dplyr::filter(., dfminput == TRUE) %>%
    	dplyr::select(., fullname, st) %>%
    	setNames(., c('Variable', 'Stationary Form')) %>%
    	xtable::xtable(., caption = 'Imported Monthly Data - Leading Economic Variables', align = 'llc') %>%
    	print(., size = 'scriptsize')
@

We additionally import quarterly data for our variables of interest. While this model can be used for nowcasting other data, here we will use import GDP, its components, as well as several other major macroeconomic variables of interest.
<<>>=
	ef$paramsDf %>%
    	# dplyr::filter(., category == 'Output') %>%
    	dplyr::filter(., freq == 'q') %>% 
    	dplyr::select(., fullname, st) %>%
    	setNames(., c('Variable', 'Stationary Form')) %>%
    	xtable::xtable(., caption = 'Imported Quarterly Data', align = 'llc') %>%
    	print(., size = 'scriptsize')
@

Finally, we import additional monthly data that may be of interest to forecast, but are not leading indicators that we will use in our principal components analysis.
<<>>=
	ef$paramsDf %>%
    	dplyr::filter(., is.na(dfminput) & freq == 'm') %>%
    	dplyr::select(., fullname, st) %>%
    	setNames(., c('Variable', 'Stationary Form')) %>%
    	xtable::xtable(., caption = 'Other Imported Monthly Data', align = 'llc') %>%
    	print(., size = 'scriptsize')
@

Most datasets have already been deseasonalized if necessary by their original source. We deseasonalize the remaining series by using the U.S. Census Bureau's seasonal adjustment package, X13-ARIMA-SEATS. We interface with it by using the \texttt{seasonal} package implementation in R (Sax and Eddelbuettel 2018).

\subsection{Time Periods}
Now we will segment the data by time periods. The imported monthly data will have ragged edges - i.e., some monthly data will be available for later months than others.

We will let $T$ denote the number of dates for which data is available for all data series. $\tau$ will denote the number of dates for which data is available for at least one data series. $T^*$ will denote number of dates up to the end-of-quarter month of the $\tau$ date. For example, suppose date $\tau$ occurs on February. The end-of-quarter month, $T^*$, will be March (since Q1 runs through the end of March). 

In other words, data will be indexed by $t = 1, 2, \dots, T, T+1, \dots, \tau, \dots, T^*$, where dates $T + 1$ through $\tau$ are the dates for which only some data are available, and dates $\tau + 1$ through $T^*$ are the dates for which no data is available up to the next quarter-ending month.
\begin{figure}[H]
\includegraphics[scale=.7]{nowcast-time}
\centering
\end{figure}
For our data, we set the dates as follows.
<<>>=
    ef$nc$timeDf %>%
		dplyr::mutate(., date = as.character(date)) %>%
    	dplyr::mutate(., time = ifelse(time == 'Tau', '$\\tau$', paste0('$', time, '$'))) %>%
    	setNames(., c('Date', '$t$')) %>%
    	xtable::xtable(., caption = 'Time Periods', align = 'lcc') %>%
		print(., sanitize.colnames.function = function(x) x, sanitize.text.function = function(x) x)
@

\subsection{Principal Components Analysis}
It is known that a large number of macroeconomic time series are highly correlated; using such covariates as regressors could naturally lead to problems with collinearity and unstable estimates. In addition, it becomes computationally burdensome to analyze data with such a large number of highly correlated variables. Instead, we use principal components analysis (PCA) to shrink our dataset in a way that allows us to retain most of the information in our original data.

Estimation of factors is derived following Stock and Watson (2008). We begin by taking our $T \times N$ data matrix of $N$ monthly leading economic variables, from time 1 through $T$. The matrix, which we denote $X$, is normalized to mean 0 and variance 0 across all columns.

The goal is to minimize the error $E$ below.
\begin{align*}
	X = F  \Lambda ' + E,\\
	\text{where $X$ is the $T \times N$ data matrix,}\\
	\text{$F$ is the $T \times N$ matrix of factors,}\\
	\text{and $\Lambda$ is the weighting matrix.}\\
\end{align*}

Estimation of factors is derived following Stock and Watson (2008).
\begin{align*}
	\widehat{\Lambda} = \text{eigenvectors of } (X'X)\\
	\widehat{F} = X \widehat{\Lambda}
\end{align*}



Once factors are derived, we select the optimal number of factors to use in predictive regressions. To do so, we use the information criteria from Bai and Ng (2002). Let $R$ refer to the number of factors used. We also include alternative specifications of the information criteria from Bai and Ng as a robustness check.
\begin{align*}
	IC(R) = MSE + R \times \frac{N+T}{NT} \times log\left(\frac{NT}{N+T}\right)
\end{align*}

<<fig.cap = 'Factor Selection'>>=
	ef$nc$screePlot
@

<<>>=
	ef$nc$screeDf %>%
    	setNames(., c('Factors (R)', 'Variance Explained', 'Pct of Total Var Explained', 'Cumulative Pct', 'MSE', 'IC1', 'IC2', 'IC3')) %>%
    	xtable::xtable(., caption = 'Factor Selection Process', align = 'lcccccccc') %>%
    	print(., size = 'scriptsize')
@

Choosing the IC-minimizing $R$ lets us choose $R = \Sexpr{ef$nc$bigR}$ factors.

<<>>=
for (i in 1:length(ef$nc$zPlots)) {
	print(ef$nc$zPlots[[i]])
}
@

Next, we perform a qualitative check of the factors. The first factor usually represents the COVID-19 shock. Typically the second factor should give us something similar to the growth rate of GDP or aggregate production, but on a monthly basis. The third and fourth factors may vary but often represent interest rates or consumption. Note that the sign direction of the factors is irrelevant to the modeling process, and they may be switched negated without consequence. 

Finally, we evaluate the components of each factor, i.e. which 
<<>>=
	ef$nc$factorWeightsDf %>%
    	head(., 5) %>%
    	xtable::xtable(., caption = 'PCA Factor Weights - Top 5 Per Factor', align = 'cccc') %>%
    	print(., size = 'scriptsize')
@



\subsection{Factor VAR}
The next step is to model the transition of the factors over time. To do so, we utilize a vector-autoregressive (VAR) process, following Stock and Watson (2016). As before, $R$ will refer to the total number of factors we extracted in the previous section, and $f^i_t$ for $i = 1, \dots, R$ will refer to the value of factor $i$ at time $t$.

We will use a VAR(1) model of the following form.
\begin{align*}
\underbrace{\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}}_{z_t}
=
B
\underbrace{\begin{bmatrix}
	f^1_{t-1}\\
	f^2_{t-1}\\
	\vdots \\
	f^R_{t-1}
\end{bmatrix}}_{z_{t-1}}
+
C
+
\underbrace{\begin{bmatrix}
v^1_t\\
v^2_t\\
\vdots\\
v^R_t
\end{bmatrix}}_{v_t},\\
\text{where $z_t$ is the $R \times 1$ matrix of time $t$ factors,}\\
\text{$B$ is the $R \times R$ coefficient matrix,}\\
\text{$C$ is the $R \times 1$ constant matrix,}\\
\text{and $v_t$ is the $R \times 1$ matrix of errors for time $t$.}
\end{align*}


We wish to estimate the coefficient matrices $B$ and $C$. This can be done via OLS estimation. We first rewrite the data as the standard linear equation,
\begin{align*}
\underbrace{\begin{bmatrix}
f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
f^1_{3} & f^2_{3} & \dots & f^R_{3}\\
\vdots & \vdots & \vdots & \vdots \\
f^1_{T} & f^2_{T} & \dots & f^R_{T}
\end{bmatrix}}_{\Gamma}
=
\underbrace{\begin{bmatrix}
1 & f^1_{1} & f^2_{1} & \dots & f^R_{1}\\
1 & f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & f^1_{T-1} & f^2_{T-1} & \dots & f^R_{T-1}
\end{bmatrix}}_{\Psi}
\underbrace{\begin{bmatrix}
C'\\
B'
\end{bmatrix}}_{\Lambda}
 +
\underbrace{\begin{bmatrix}
v^1_2 & v^2_2 & \dots & v^R_2\\
v^1_3 & v^2_3 & \dots & v^R_3\\
\vdots\\
v^1_T & v^2_T & \dots & v^R_T\\
\end{bmatrix}}_{V},\\
\text{where $\Gamma$ is the $T-1 \times R$ dependent data matrix,}\\
\text{$\Psi$ is the $T-1 \times R+1$ independent data matrix,}\\
\text{$\Lambda$ is the $R+1 \times R$ matrix of coefficient weightings,}\\
\text{and $V$ is the $T-1 \times R$ matrix of residuals.}
\end{align*}
The coefficient matrix $\Lambda$ can be estimated by the standard OLS estimator.
\begin{align*}
\widehat{\Lambda} = (\Psi' \Psi)^{-1} (\Psi'\Gamma)
\end{align*}
It can then be partitioned to calculate $\widehat{B}'$ and $\widehat{C}'$, which can then be transposed to derive our estimates of the original coefficient matrices B and C, $\widehat{B}$ and $\widehat{C}$.

The estimated coefficients in $\widehat{B}$ and $\widehat{C}$ are shown below.
<<>>=
	ef$nc$varCoefDf %>%
    	tidyr::pivot_longer(., -coefname) %>%
    	tidyr::pivot_wider(., names_from = coefname) %>%
    	xtable::xtable(., caption = 'Factor VAR Coefficients') %>%
    	print(.)
@


Finally, we perform a qualitative check of the fitted values and residuals. It is important that factors that are predictable --- i.e., factors 2 and 3, since they represent output --- have a good fit. Since factor 1 represents the COVID-19 shock, we should expect that the fit is poor; such a shock should not be predictable simply from the time dynamics of the factors; so if the fit is good, our model is likely overfitted.
<<>>=
for (i in 1:length(ef$nc$varFittedPlots)) {
	print(ef$nc$varFittedPlots[[i]])
}
@


<<fig.cap = 'Factor VAR Residuals'>>=
print(ef$nc$varResidPlot)
@

Additionally, we expect residuals of the nowcast. Goodness-of-fit statistics are shown below.
<<>>=
ef$nc$varGofDf %>%
    xtable::xtable(., caption = 'DFM Goodness of Fit', digits = 8) %>%
    print(.)
@



\subsection{Dynamic Factor Models}
Now let us consider again the monthly leading economic variables which were include in the principal components analysis. We will model these as dynamic factor models (DFMs), i.e. - they are regressed on the factor variables derived from earlier. As before, let $x^i_t$ refer to the time $t$ value for monthly variable $x^i$, where $i = 1, \dots, N$.

The factor models take the following form:
\begin{align*}
\underbrace{\begin{bmatrix}
	x^1_t\\
	x^2_t\\
	\vdots \\
	x^N_t
\end{bmatrix}}_{y_t}
=
A
\underbrace{\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}}_{z_t}
+
D 
+
\underbrace{\begin{bmatrix}
	w^1_t\\
	w^2_t\\
	\vdots\\
	w^N_t
\end{bmatrix}}_{w_t},\\
\text{where $y_t$ is the $N \times 1$ vector of monthly variables at time $t$,}\\
\text{$A$ is the $N \times R$ coefficient matrix,}\\
\text{$z_t$ is the $R \times 1$ vector of factors at time $t$,}\\
\text{$D$ is the $N \times 1$ constant matrix,}\\
\text{and $w_t$ is the $N \times 1$ vector of errors at time $t$.}\\
\end{align*}
We wish to estimate the coefficient matrices $A$ and $D$. As before, we can do this by estimating this as an OLS equation, writing the data matrices as follows

\begin{equation}
\underbrace{\begin{bmatrix}
x^1_{2} & x^2_{2} & \dots & x^N_{2}\\
x^1_{3} & x^2_{3} & \dots & x^N_{3}\\
\vdots & \vdots & \vdots & \vdots \\
x^1_{T} & x^2_{T} & \dots & x^N_{T}\\
\end{bmatrix}}_{\Phi}
=
\underbrace{\begin{bmatrix}
1 & f^1_{2} & f^2_{2} & \dots & f^R_{2}\\
1 & f^1_{3} & f^2_{3} & \dots & f^R_{3}\\
\vdots & \vdots & \vdots & \vdots & \vdots \\
1 & f^1_{T} & f^2_{T} & \dots & f^R_{T}\\
\end{bmatrix}}_{\Omega}
\begin{bmatrix}
D'\\
A'
\end{bmatrix}
 +
\underbrace{\begin{bmatrix}
w^1_2 & w^2_2 & \dots & w^R_2\\
w^1_3 & w^2_3 & \dots & w^R_3\\
\vdots\\
w^1_T & w^2_T & \dots & w^R_T\\
\end{bmatrix}}_{W}
\end{equation}

However, estimation of $A$ and $D$ with the standard OLS estimator is likely to lead to significant overfitting, particularly if the matrix of $f$ factors is particularly high-dimensional. We will instead use an elastic net process to regularize covariate selection (Zou and Hastie 2005). Hyperparameters of the elastic net are chosen through a cross-validated MAE-minimization process.
% As before we can estimate $A$ and $D$ with the standard OLS estimator.
% \begin{align*}
% \begin{bmatrix}
% 	\widehat{A}\\
% 	\widehat{D}
% \end{bmatrix} =
% (\Omega' \Omega)^{-1} (\Omega'\Phi)
% \end{align*}

Estimated coefficients for $\widehat{A}$ and $\widehat{D}$ are below.
<<>>= 
ef$nc$dfmCoefDf %>% tidyr::pivot_longer(., -coefname) %>% tidyr::pivot_wider(., names_from = coefname) %>%
    	xtable::xtable(., caption = 'Estimated DFM Coefficients') %>%
    	print(.)
@

We perform a qualitative check of the in-sample fit, as well as a quantitative review of the goodness-of-fit. The graphs of the fitted plots are located in the appendix.
<<>>=
	ef$nc$dfmGofDf %>%
    	xtable::xtable(., caption = 'DFM Goodness of Fit', digits = 8) %>%
    	print(.)
@




\subsection{State-Space Setup}
Now, combining our equations for the DFM and the VAR, we have the below system.
\begin{align*}
z_t = B z_{t-1} + Cx + v_t\\
y_t = A z_t + w_t
\end{align*}

This system is now fully specified and in state-space form. The first equation is our state (or transition) equation. The second equation is our measurement equation. 

We use our estimated values $B$, $C$, $A$, and $D$ calculated in our previous two sections. To run the Kalman Filter, we will want to create the actual data matrices for $z_t$ and $y_t$. $z_t$ can be constructed as before, using data for factors from time 1 through $T$. However, unlike in the previous two sections, we will want to create $y_t$ matrices not for just time periods 1 through $T$, but now for time periods $1$ through $\tau$. Elements in $y_t$ may be set to any value for missing observations; the process of Kalmam filtration will render this choice irrelevant.

Specifically, we construct the matrices below.
\begin{align*}
z_t
=
\begin{bmatrix}
	f^1_{t}\\
	f^2_{t}\\
	\vdots \\
	f^R_{t}
\end{bmatrix}, \forall t \in 1, \dots, T\\
y_t
=
\begin{bmatrix}
	\text{$x^1_{t}$ if available, otherwise 0}\\
	\text{$x^2_{t}$ if available, otherwise 0}\\
	\vdots \\
	\text{$x^N_{t}$ if available, otherwise 0}\\
\end{bmatrix}, \forall t \in 1, \dots, \tau
\end{align*}

For Kalman filtration, we also require an assumed distribution on $v_t$ and $w_t$. We assume that $v_t$ is distributed normally with mean 0 and constant diagonal covariance matrix denoted $Q$, with diagonal entries calculated by taking the average squared values of the residuals of the VAR.

We also assume $w_t$ is distributed normally with mean $0$. However, we no longer specify the covariance matrix as constant, but as the time-dependent matrices $R_t$. For $t \in 1, \dots, T+1$, we let $R_t$ be a diagonal covariance matrix with diagonal entries calcualted by taking the average squared values of the residuals of the DFM. For $t \in T+1, \dots, \tau$, we let the diagonal elements of $R_t$ be equal to infinity if the corresponding element of $y_t$ is missing for that time period; and equal to the average squared value of the residual if otherwise.
\begin{align*}
	v_t \sim \mathcal{N}(0, Q)\\
	w_t \sim \mathcal{N}(0, R_t)
\end{align*}

\subsection{Kalman Filtration}
Now that our state-space model is fully specified, we can begin the Kalman filter recursions.
\begin{align*}
	z_t = B z_{t-1} + Cx + v_t\\
	y_t = A z_t + Dx + w_t\\
	v_t \sim \mathcal{N}(0, Q)\\
	w_t \sim \mathcal{N}(0, R_t)
\end{align*}

To solve this programmatically, we will need the previously estimated matrices $A$, $B$, $C$, and $D$; the matrices $z_t$ from 1 through $T$; the matrices $y_t$ from 1 through $\tau$; the covariance matrix $Q$; and finally, the covariance matrices $R_t$ from 1 through $\tau$.

We initialize the Kalman filter with the following standard assumptions. 
\begin{align*}
	\vv{z}_{0|0} = 0\\
	\vv{CovZ} = 0
\end{align*}

Now for $t = 1, \dots, \tau$, we iterate through the Kalman filter recursions and iteratively calculate the values below.
\begin{align*}
	\vv{z}_{t|t-1} &= B \vv{z}_{t-1|t-1} + C\\
	\vv{CovZ}_{t|t-1} &= B \vv{CovZ}_{t-1|t-1} + Q\\
	\vv{y}_{t|t-1} &= A \vv{z}_{t|t-1} + D\\
	\vv{CovY}_{t|t-1} &= A \vv{CovZ}_{t|t-1} A' + R_t\\
	P_t &= \vv{CovZ}_{t|t-1} A' \vv{CovY}^{-1}_{t|t-1}\\
	\vv{z}_{t|t} &= \vv{z}_{t|t-1} + P_t (\vv{y}_t - \vv{y}_{t|t-1})\\
	\vv{CovZ}_{t|t} &= \vv{CovZ}_{t|t-1} - P_t (\vv{CovY}_{t|t-1}) P'_t
\end{align*}
Note that the during recursions $T + 1, \dots \tau$, the infinite values in the $R_t$ matrix will cause infinite values in the $\vv{CovY}_{t|t-1}$ matrix. This may prevent standard computational methods from computing the inverse of the matrix needed in the step for calculation of $\vv{CovZ}_{t|t}$. Alternative methods, such as a Cholesky decomposition before inversion, are used to subvert this problem.

The Kalman filter allows us to recover all the time $t$ conditional state matrices $z_{t|t}$ that have been adjusted for information from the monthly datasets. However, of more interest to us is the value of the state matrices when conditioned on all data available at time $\tau$, $z_{t|\tau}$. This can be recovered by using the Kalman smoother.

Recursively iterating over $t = \tau - 1, \dots, 1$, we calculate the following values. 
\begin{align*}
	S_{t} &= \vv{CovZ}_{t|t} B' \vv{CovZ}^{-1}_{t + 1|t}\\
	\vv{z}_{t|\tau} &= \vv{z}_{t|t} + S_t (\vv{z}_{t+1|\tau} - \vv{z}_{t + 1|t})\\
	\vv{CovZ}_{t|\tau} &= \vv{CovZ}_{t|t} - S_t(\vv{CovZ}_{t + 1|t} - \vv{CovZ}_{t + 1|\tau})S'_t
\end{align*}
These values $\vv{z}_{t|\tau}$ will serve as our estimates of the state variables (i.e., the PCA factors) from time 1 through $\tau$.

Finally, we want to forecast the the state vector $z_{t|\tau}$ for $t = \tau + 1, \dots, T^*$. This can be done through the typical Kalman filter forecasting step.

Recursively iterating over $t = \tau + 1, \dots, T^*$, we calculate the following values.
\begin{align*}
	\vv{z}_{t|\tau} &= B \vv{z}_{t-1|\tau} + C\\
	\vv{CovZ}_{t|\tau} &= B \vv{CovZ}_{t-1|\tau} B'+ Q\\
	\vv{y}_{t|\tau} &= A \vv{z}_{t|\tau} + D\\
	\vv{CovY}_{t|\tau} &= A \vv{CovZ}_{t|\tau} A' + R_0
\end{align*}
Combining the calculations for $\vv{z}_{t|\tau}$ with the ones derived from the Kalman smoother, we will now be able to obtain the full time series for the factors from time 1 through time $T^*$.

<<>>=
    for (i in 1:length(ef$nc$kfPlots)) {
    	print(ef$nc$kfPlots[[i]])
    }
@

% \subsection{Nowcasting Monthly Covariates}
% Now that we have our Kalman-smoothed factors from time $1$ through time $T^*$, we will be able to use these as covariates to model any monthly time series that we have. 

\subsection{Nowcast Step}
Now that we have our Kalman-smoothed and forecasted factors from time $1$ through time $T^*$, we will be able to use these as covariates to model any monthly time series that we have.

We begin by aggregating these monthly factors into quarterly data by taking a simple monthly average for each factor over each quarter.

In this section, we will use these now-quarterly factors to forecast our quarterly subcomponents of GDP. As discussed in the data import section, these have been transformed for stationarity, typically by taking a log-difference.
<<>>=
ef$paramsDf %>%
    	dplyr::filter(., category == 'Output' & nowcast == 'dfm.q') %>%
    	dplyr::transmute(., Variable = fullname) %>%
    	xtable::xtable(., caption = 'Quarterly Data Covariates') %>%
    	print(., size = 'scriptsize')
@
We will notate each of these gdp subcomponents as $y^i$ and $M$ as the total number of covariates, so that $i = 1, \dots, M$.

Note that many higher-level components of GDP (including GDP itself) are \textit{not} forecasted directly in this step; these will be forecasted later by aggregating their subcomponents.

Now we will specify that these GDP subcomponents follow a DFM-AR(1) model; i.e., they will be functions of the monthly-aggregated factors as well as the first lag of themselves.
\begin{align*}
	y^i_t
	=
	\beta
	\begin{bmatrix}
	1 \\
	y^i_{t-1}\\
	f^1_t \\
	\vdots\\
	f^R_t
	\end{bmatrix}
	+
	e_t
\end{align*}
The $\beta$ coefficients can be estimated with a typical OLS process, where the training data is constituted of the $y^i_t$ quarterly covariates, the lagged quarterly covariates $y^i_{t-1}$, and the quarterly-aggregated factor variables. The data is cut off at the date for which the any data on the quarterly covariates are missing.

After estimation, we then use the same model to forecast forward the $y^i_t$ quarterly, up through time $\tau$. 
The forecasted results are as follows.
<<>>=
ef$ncpred0$q$st %>%
    	tidyr::pivot_longer(., -date, names_to = 'varname') %>%
    	dplyr::inner_join(., filter(ef$paramsDf, category == 'Output')[, c('varname', 'fullname')], by = 'varname') %>%
    	dplyr::mutate(., date = paste0(year(date), 'Q', quarter(date))) %>%
    	dplyr::select(., -varname) %>%
    	tidyr::pivot_wider(., values_from = value, names_from = date) %>%
    	dplyr::rename(., 'Variable' = fullname) %>%
    	xtable::xtable(., caption = 'DFM-AR(1) Forecasted GDP Subcomponents') %>%
    	print(., size = 'scriptsize')
@

We can then backtransform the data so that the units are in base values. After backtransformation, we are ready to aggregate these up to higher-level GDP components. In particular, we calculate the variables below.
<<>>=
ef$paramsDf %>%
    	dplyr::filter(., category == 'Output' & nowcast == 'calc') %>%
    	dplyr::transmute(., Variable = fullname) %>%
    	xtable::xtable(., caption = 'Summable Quarterly Data Covariates') %>%
    	print(., size = 'scriptsize')
@
These are calculated using the standard GDP aggregation equations, e.g., net exports = exports - imports, and so on.
Finally, we convert these into annualized percentage change, as this is the standard format in which GDP subcomponents are reported in. The results are reported in the next section.

We also use a similar DFM-AR(1) specification to forecast out other economic variables of interest, with results reported in the next section.


\section{Results}
Our final nowcasts of GDP and its subcomponents are below. All units are reported in terms of annualized percentage change (seasonally adjusted) except for change in private inventories and net exports, which are reported in terms of billions of real 2012 dollars.
<<>>=
ef$ncpred$q$d1 %>%
	tidyr::pivot_longer(., -date, names_to = 'varname') %>%
	dplyr::inner_join(filter(ef$paramsDf, category == 'Output')[, c('varname', 'fullname')], ., by = 'varname') %>%
	# Add tabs
    rowwise(.) %>%
    dplyr::mutate(., fullname = paste0(paste0(rep(' ', str_count(fullname, ':')), collapse = ''), fullname)) %>%
    dplyr::mutate(., fullname1 = paste0('\\hspace{',str_count(fullname, ':') * 8, 'mm} ')) %>%
    dplyr::mutate(., fullname2 = sub('.*\\:', '', fullname)) %>%
    dplyr::mutate(., fullname2 = str_replace(fullname2, coll('&'), '\\&')) %>%
    dplyr::mutate(., fullname2 = ifelse(str_count(fullname, ':') == 0, paste0('\\textbf{', fullname2, '}'), fullname2)) %>%
    dplyr::mutate(., fullname = paste0(fullname1, fullname2)) %>%
    dplyr::select(., -fullname1, -fullname2) %>%
    dplyr::ungroup(.) %>%
    # End
	dplyr::mutate(., date = paste0(year(date), 'Q', quarter(date))) %>%
	dplyr::select(., -varname) %>%
	tidyr::pivot_wider(., values_from = value, names_from = date) %>%
	dplyr::rename(., 'Variable' = fullname) %>%
	xtable::xtable(., caption = 'Nowcasts for GDP and Subcomponents (Annualized Percent Change)') %>%
	print(., size = '\\fontsize{10pt}{12pt}\\selectfont', sanitize.text.function = function(x) x, add.to.row = list(pos = list(nrow(.)), command = '\\hline \n \\textit{apchg} = annualized \\% change \n'), hline.after = c(-1, 0))
@

We also report nowcasts of other variables of interest, including other quarterly variables, as well as all monthly variables used in this process. Note that because these variables are ragged-edge in nature, there may be "blank" columns. These columns simply indicate that historical data already exists for those columns. Only nowcasts produced by the model are reported.
<<>>=
ef$ncpred$q$d1 %>%
	tidyr::pivot_longer(., -date, names_to = 'varname') %>%
    dplyr::inner_join(filter(ef$paramsDf, category != 'Output')[, c('varname', 'units', 'd1', 'fullname')], ., by = 'varname') %>%
    dplyr::mutate(., fullname = ifelse(d1 != 'base', paste0(fullname, ' \\textit{\\footnotesize\\textcolor{gray}{(', d1, ')}}'), fullname)) %>%
    dplyr::mutate(., fullname = str_replace(fullname, coll('&'), '\\&')) %>%
	dplyr::mutate(., date = paste0(year(date), 'Q', quarter(date))) %>%
	dplyr::select(., -varname, -d1, -units) %>%
	tidyr::pivot_wider(., values_from = value, names_from = date) %>%
	dplyr::rename(., 'Variable' = fullname) %>%
	xtable::xtable(., caption = 'Nowcasts for Other Quarterly Variables') %>%
	print(., size = '\\fontsize{11pt}{13pt}\\selectfont')
@
<<>>=
ef$ncpred$m$d1 %>%
    tidyr::pivot_longer(., -date, names_to = 'varname') %>%
    dplyr::inner_join(filter(ef$paramsDf, category != 'Output')[, c('varname', 'units', 'd1', 'fullname')], ., by = 'varname') %>%
    dplyr::mutate(., fullname = ifelse(d1 != 'base', paste0(fullname, ' \\textit{\\footnotesize\\textcolor{gray}{(', d1, ')}}'), fullname)) %>%
    dplyr::mutate(., fullname = str_replace(fullname, coll('&'), '\\&')) %>%
    dplyr::mutate(., date = paste0(year(date), 'M', month(date))) %>%
    dplyr::select(., -varname, -d1, -units) %>%
    tidyr::pivot_wider(., values_from = value, names_from = date) %>%
    dplyr::rename(., 'Variable' = fullname) %>%
    xtable::xtable(., caption = 'Nowcasts for All Monthly Variables') %>%
	print(., size = '\\fontsize{10pt}{12pt}\\selectfont', sanitize.text.function = function(x) x, add.to.row = list(pos = list(nrow(.)), command = '\\hline \n \\textit{apchg} = annualized \\% change \n'), hline.after = c(-1, 0))
@


\appendix
\appendixpage
\addappheadtotoc

\section{DFM Fitted Plots}
<<>>=
for (i in 1:length(ef$nc$dfmFittedPlots)) {
	print(ef$nc$dfmFittedPlots[[i]])
}

@


\end{document}